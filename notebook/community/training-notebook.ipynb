{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujgpu_u1gTRT"
      },
      "source": [
        "```\n",
        "token space layout\n",
        "0-17 - codebooks channel 1\n",
        "18-35 - codebooks channel 2\n",
        "36 - control codes\n",
        "37 - genre\n",
        "38 - artist\n",
        "39 - length\n",
        "40 - tempo\n",
        "41 - mood\n",
        "42 - instruments\n",
        "-63 - ?\n",
        "\n",
        "control lane:\n",
        "0 - nothing\n",
        "1-6 - ~0/3/6/12/24/48 seconds before end\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzwu1y0Maldn",
        "outputId": "a218010d-8b49-44cb-a0dc-01cd45152114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9GOv1oibnT"
      },
      "source": [
        "# dataset prep test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0j62UuLelZIc",
        "outputId": "0e73adce-74f1-43b2-dd3e-1d3bc27ab34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting descript-audio-codec\n",
            "  Downloading descript_audio_codec-1.0.0-py3-none-any.whl (26 kB)\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.7.6-py2.py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argbind>=0.3.7 (from descript-audio-codec)\n",
            "  Downloading argbind-0.3.7.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting descript-audiotools>=0.7.2 (from descript-audio-codec)\n",
            "  Downloading descript_audiotools-0.7.2-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from descript-audio-codec)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from descript-audio-codec) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from descript-audio-codec) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from descript-audio-codec) (2.0.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from descript-audio-codec) (4.66.1)\n",
            "Collecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from argbind>=0.3.7->descript-audio-codec) (6.0.1)\n",
            "Collecting docstring-parser (from argbind>=0.3.7->descript-audio-codec)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.12.1)\n",
            "Collecting pyloudnorm (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (6.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (1.10.1)\n",
            "Collecting julius (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpy (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (7.34.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (13.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (3.7.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.10.1)\n",
            "Collecting pystoi (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading pystoi-0.3.3.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-stoi (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading torch_stoi-0.1.2.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flatten-dict (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting markdown2 (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\n",
            "Collecting randomname (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading randomname-0.2.1.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting protobuf<3.20,>=3.9.2 (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (2.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->descript-audio-codec) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->descript-audio-codec) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->descript-audio-codec) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->descript-audio-codec) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->descript-audio-codec) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->descript-audio-codec) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->descript-audio-codec) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->descript-audio-codec) (16.0.6)\n",
            "Requirement already satisfied: six<2.0,>=1.12 in /usr/local/lib/python3.10/dist-packages (from flatten-dict->descript-audiotools>=0.7.2->descript-audio-codec) (1.16.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->descript-audio-codec) (2.1.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.56.4)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.3.6)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.0.5)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->descript-audiotools>=0.7.2->descript-audio-codec) (1.15.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (2.8.2)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from pyloudnorm->descript-audiotools>=0.7.2->descript-audio-codec) (0.18.3)\n",
            "Collecting fire (from randomname->descript-audiotools>=0.7.2->descript-audio-codec)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->descript-audiotools>=0.7.2->descript-audio-codec) (3.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->descript-audio-codec) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (0.41.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->descript-audiotools>=0.7.2->descript-audio-codec) (2.21)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->descript-audiotools>=0.7.2->descript-audio-codec) (0.1.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.39.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (2.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->randomname->descript-audiotools>=0.7.2->descript-audio-codec) (2.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (3.2.2)\n",
            "Building wheels for collected packages: argbind, ffmpy, julius, pystoi, randomname, torch-stoi, fire\n",
            "  Building wheel for argbind (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for argbind: filename=argbind-0.3.7-py2.py3-none-any.whl size=11703 sha256=17f240d64ad853a2bc2e5482fe276f9ca864de473a69449dc89a73d567a17dcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/dc/86/4cb83c8d7522e6fb03e77f7c64acdce3b7b9663c580d0ce720\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=d180711d58c21a61136e68e60376050ec1739f52532f827fedc8ab1f625ad18e\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21868 sha256=8a6c887d4e5ade213cf41aba4221b4a2ec7cff70118f5fb83a3e5671d5280f6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n",
            "  Building wheel for pystoi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pystoi: filename=pystoi-0.3.3-py2.py3-none-any.whl size=7778 sha256=8130feb27c36987a32016ff0d8bccc23080c055a97bf5f6e7a34281300117bea\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/ca/9e/5b5d6e5e109322303b50d21918ad2bd7d50a2a0775c11e08e8\n",
            "  Building wheel for randomname (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for randomname: filename=randomname-0.2.1-py3-none-any.whl size=89194 sha256=2571adbf58737115b03cc9e57db577bde7b7a0f5769ada3fd11896c4aa0f0084\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/50/8a/25f3820d26a431ffed1834d72ff2eb349123cf2b44c5a45727\n",
            "  Building wheel for torch-stoi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-stoi: filename=torch_stoi-0.1.2-py3-none-any.whl size=6184 sha256=38eced48fa073f17da41bcfb15f86247042481b28bd737e2c497c7122e28b739\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/9d/22/fba987c95b98db12db34eb5b1172e5478fda82da093ac62615\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=a9355c2063f6b1c197d45f1b21bc9f144573949ceeb546957344dadba3c4a3c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built argbind ffmpy julius pystoi randomname torch-stoi fire\n",
            "Installing collected packages: ffmpy, brotli, websockets, pycryptodomex, protobuf, mutagen, markdown2, jedi, flatten-dict, fire, einops, docstring-parser, yt-dlp, randomname, pystoi, pyloudnorm, argbind, torch-stoi, julius, descript-audiotools, descript-audio-codec\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed argbind-0.3.7 brotli-1.0.9 descript-audio-codec-1.0.0 descript-audiotools-0.7.2 docstring-parser-0.15 einops-0.6.1 ffmpy-0.3.1 fire-0.5.0 flatten-dict-0.4.2 jedi-0.19.0 julius-0.2.7 markdown2-2.4.10 mutagen-1.47.0 protobuf-3.19.6 pycryptodomex-3.18.0 pyloudnorm-0.1.1 pystoi-0.3.3 randomname-0.2.1 torch-stoi-0.1.2 websockets-11.0.3 yt-dlp-2023.7.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install descript-audio-codec yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omWC30qzlbNP",
        "outputId": "2cf922db-e1a3-4713-fd75-16bc429c8c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'RWKV-infctx-trainer'...\n",
            "remote: Enumerating objects: 7527, done.\u001b[K\n",
            "remote: Counting objects: 100% (1075/1075), done.\u001b[K\n",
            "remote: Compressing objects: 100% (401/401), done.\u001b[K\n",
            "remote: Total 7527 (delta 743), reused 965 (delta 670), pack-reused 6452\u001b[K\n",
            "Receiving objects: 100% (7527/7527), 29.75 MiB | 15.84 MiB/s, done.\n",
            "Resolving deltas: 100% (5156/5156), done.\n",
            "Cloning into 'json2binidx_tool'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 53 (delta 12), reused 7 (delta 7), pack-reused 36\u001b[K\n",
            "Receiving objects: 100% (53/53), 1003.34 KiB | 2.01 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "Collecting lightning==2.0.5\n",
            "  Downloading lightning-2.0.5-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed==0.10.0\n",
            "  Downloading deepspeed-0.10.0.tar.gz (836 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m836.6/836.6 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (2.8.5)\n",
            "Collecting jsonargparse\n",
            "  Downloading jsonargparse-4.24.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.7/182.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lm-dataformat\n",
            "  Downloading lm_dataformat-0.0.20-py3-none-any.whl (5.8 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.9-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting papermill\n",
            "  Downloading papermill-2.4.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (3.1.2)\n",
            "Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (6.0.1)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning==2.0.5)\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<4.0,>=2.2.1 (from lightning==2.0.5)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (8.1.7)\n",
            "Collecting croniter<1.5.0,>=1.3.0 (from lightning==2.0.5)\n",
            "  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning==2.0.5)\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning==2.0.5)\n",
            "  Downloading deepdiff-6.4.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<2.0,>=0.92.0 (from lightning==2.0.5)\n",
            "  Downloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (2023.6.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning==2.0.5)\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.37 (from lightning==2.0.5)\n",
            "  Downloading lightning_cloud-0.5.37-py3-none-any.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.7.0 (from lightning==2.0.5)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (23.1)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (5.9.5)\n",
            "Collecting pydantic<2.0.0,>=1.7.4 (from lightning==2.0.5)\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart<2.0,>=0.0.5 (from lightning==2.0.5)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (2.31.0)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (13.5.2)\n",
            "Collecting starlette (from lightning==2.0.5)\n",
            "  Downloading starlette-0.31.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning==2.0.5)\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (2.0.1+cu118)\n",
            "Collecting torchmetrics<2.0,>=0.7.0 (from lightning==2.0.5)\n",
            "  Downloading torchmetrics-1.1.1-py3-none-any.whl (763 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.4/763.4 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (4.66.1)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (4.7.1)\n",
            "Requirement already satisfied: urllib3<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (2.0.4)\n",
            "Collecting uvicorn<2.0 (from lightning==2.0.5)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (1.6.2)\n",
            "Requirement already satisfied: websockets<13.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.5) (11.0.3)\n",
            "Collecting pytorch-lightning (from lightning==2.0.5)\n",
            "  Downloading pytorch_lightning-2.0.8-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.0/727.0 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hjson (from deepspeed==0.10.0)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.0) (9.0.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from jsonargparse) (0.15)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse)\n",
            "  Downloading typeshed_client-2.3.0-py3-none-any.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.6/581.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines (from lm-dataformat)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting ujson (from lm-dataformat)\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from lm-dataformat)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.34-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.6/188.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.30.0-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.19.6)\n",
            "Collecting ansiwrap (from papermill)\n",
            "  Downloading ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.10/dist-packages (from papermill) (5.9.2)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from papermill) (0.8.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from papermill) (0.4)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from papermill) (8.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning==2.0.5) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning==2.0.5) (2.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning==2.0.5) (2023.3)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning==2.0.5)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi<2.0,>=0.92.0->lightning==2.0.5) (3.7.1)\n",
            "Collecting starlette (from lightning==2.0.5)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning==2.0.5)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning==2.0.5)\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning==2.0.5)\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<5.0->lightning==2.0.5) (2.1.3)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from lightning-cloud>=0.5.37->lightning==2.0.5) (2.3.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill) (5.3.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill) (4.19.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning==2.0.5) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning==2.0.5) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning==2.0.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning==2.0.5) (2.16.1)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning==2.0.5) (2.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.5) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.5) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.5) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning==2.0.5) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning==2.0.5) (16.0.6)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse) (6.0.1)\n",
            "Collecting h11>=0.8 (from uvicorn<2.0->lightning==2.0.5)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting textwrap3>=0.9.2 (from ansiwrap->papermill)\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi<2.0,>=0.92.0->lightning==2.0.5) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi<2.0,>=0.92.0->lightning==2.0.5) (1.1.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.9.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (23.2.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (6.3.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill) (3.10.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning==2.0.5) (0.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning==2.0.5) (1.3.0)\n",
            "Building wheels for collected packages: deepspeed, pathtools\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.10.0-py3-none-any.whl size=877478 sha256=315aa0541e50bc078342e4727174336510cdf263d04f435ad2e4b810004a13ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/7b/3f/2807682bad2fba40ed888e6309597a5fda545ab30964c835aa\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=7c7573e1f8cdaa559e33175d79a0ff68e2355433e1c989fa82e0b1e36835fd58\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built deepspeed pathtools\n",
            "Installing collected packages: tokenizers, textwrap3, sentencepiece, safetensors, python-editor, pathtools, ninja, hjson, zstandard, xxhash, ujson, typeshed-client, smmap, setproctitle, sentry-sdk, readchar, python-multipart, pydantic, ordered-set, lightning-utilities, jsonlines, jsonargparse, h11, ftfy, docker-pycreds, dill, blessed, backoff, ansiwrap, uvicorn, starlette, multiprocess, lm-dataformat, inquirer, huggingface-hub, gitdb, deepdiff, dateutils, croniter, arrow, transformers, starsessions, GitPython, fastapi, wandb, lightning-cloud, datasets, papermill, torchmetrics, pytorch-lightning, lightning, deepspeed\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.1\n",
            "    Uninstalling pydantic-2.2.1:\n",
            "      Successfully uninstalled pydantic-2.2.1\n",
            "Successfully installed GitPython-3.1.34 ansiwrap-0.8.4 arrow-1.2.3 backoff-2.2.1 blessed-1.20.0 croniter-1.4.1 datasets-2.14.4 dateutils-0.6.12 deepdiff-6.4.1 deepspeed-0.10.0 dill-0.3.7 docker-pycreds-0.4.0 fastapi-0.103.1 ftfy-6.1.1 gitdb-4.0.10 h11-0.14.0 hjson-3.1.0 huggingface-hub-0.16.4 inquirer-3.1.3 jsonargparse-4.24.1 jsonlines-4.0.0 lightning-2.0.5 lightning-cloud-0.5.37 lightning-utilities-0.9.0 lm-dataformat-0.0.20 multiprocess-0.70.15 ninja-1.11.1 ordered-set-4.1.0 papermill-2.4.0 pathtools-0.1.2 pydantic-1.10.12 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.8 readchar-4.0.5 safetensors-0.3.3 sentencepiece-0.1.99 sentry-sdk-1.30.0 setproctitle-1.3.2 smmap-5.0.0 starlette-0.27.0 starsessions-1.3.0 textwrap3-0.9.2 tokenizers-0.13.3 torchmetrics-1.1.1 transformers-4.33.0 typeshed-client-2.3.0 ujson-5.8.0 uvicorn-0.23.2 wandb-0.15.9 xxhash-3.3.0 zstandard-0.21.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/RWKV/RWKV-infctx-trainer.git\n",
        "!git clone https://github.com/Abel2076/json2binidx_tool.git\n",
        "!pip install lightning==2.0.5 deepspeed==0.10.0 datasets transformers ninja numexpr jsonargparse 'jsonargparse[signatures]' lm-dataformat ftfy sentencepiece tokenizers wandb papermill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv3Z82dpVbmo",
        "outputId": "ce8fd2c6-5951-4f78-8987-0d888469ff04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/RWKV-infctx-trainer/RWKV-v5/src/data.py\n"
          ]
        }
      ],
      "source": [
        "#@title patch data.py\n",
        "\n",
        "%%writefile /content/RWKV-infctx-trainer/RWKV-v5/src/data.py\n",
        "\n",
        "from lightning import LightningDataModule\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import wandb\n",
        "from datasets import load_from_disk, load_dataset, Dataset\n",
        "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
        "from multiprocessing import cpu_count\n",
        "num_cpus = cpu_count()\n",
        "num_workers = cpu_count() if cpu_count() < 8 else 8\n",
        "\n",
        "# Get the script directory\n",
        "import os\n",
        "SRC_DIR = os.path.dirname(os.path.realpath(__file__))\n",
        "\n",
        "# World tokenizer\n",
        "from .dataflow.trie_tokenizer import world_tokenizer_encode\n",
        "import numpy as np\n",
        "\n",
        "# We have to extract out the prepare function to be \"outside the class\"\n",
        "# else it will not be hashed / serialized properly, and will produce the following error:\n",
        "#\n",
        "# ```\n",
        "# Parameter 'function'=<function RWKVDataModule.prepare_data.<locals>.map_tokenizer at 0x7f7672c5e340> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
        "# ```\n",
        "def prepare_data_static(**kargs):\n",
        "\n",
        "    # Check if skip_datapath_setup is enabled\n",
        "    # useful for extra large datasets\n",
        "    if kargs[\"skip_datapath_setup\"] == True:\n",
        "        return\n",
        "\n",
        "    # Special handling of world_add_endoftext_token (if enabled)\n",
        "    if kargs[\"world_add_endoftext_token\"]:\n",
        "        world_add_endoftext_token = True\n",
        "    else:\n",
        "        world_add_endoftext_token = False\n",
        "\n",
        "    # Source data processing\n",
        "    if kargs[\"source\"] is not None:\n",
        "        if kargs[\"tokenizer\"] is None:\n",
        "            raise ValueError('Tokenizer must be specified if source is specified')\n",
        "\n",
        "        # Special handling for binidx\n",
        "        #--------------------------------\n",
        "\n",
        "        if kargs[\"tokenizer\"] == \"binidx\":\n",
        "            from .dataflow.binidx import MMapIndexedDataset\n",
        "\n",
        "            # Load the MMapIndexedDataset from the source path\n",
        "            mmap_dataset = MMapIndexedDataset(kargs[\"source\"])\n",
        "            mmap_dataset_len = mmap_dataset.__len__()\n",
        "\n",
        "            # Torch dataset generator wrapper\n",
        "            def gen():\n",
        "                for idx in range(mmap_dataset_len):\n",
        "                    # cast to supported types, note that np.int32 limit is 2,147,483,647\n",
        "                    # - so unless we have a tokenizer that exceeds this, it should be ok\n",
        "                    tokens = np.array(mmap_dataset.get(idx), dtype=np.int32)\n",
        "                    tokens = tokens.reshape(-1, kargs[\"multitoken_width\"])\n",
        "                    yield {\n",
        "                        'input_ids': tokens,\n",
        "                        'token_type_ids': [0] * len(tokens),\n",
        "                        'attention_mask': [1] * len(tokens)\n",
        "                    }\n",
        "\n",
        "            # Load the huggingface dataset from the generator\n",
        "            src_dataset = Dataset.from_generator(gen)\n",
        "\n",
        "            # Train/test split\n",
        "            test_split = kargs[\"test_split\"]\n",
        "            # The minimum test size is 1, if not we will get errors in the trainer?\n",
        "            if test_split <= 0 or test_split <= 0.0:\n",
        "                test_split = 1\n",
        "            split_dataset = src_dataset.train_test_split(\n",
        "                test_size=test_split,shuffle=kargs[\"test_split_shuffle\"],\n",
        "                seed=42 #Fixed seed, to prevent train/test reshuffling between test runs\n",
        "            )\n",
        "\n",
        "            # Save the dataset to disk\n",
        "            split_dataset.save_to_disk(kargs[\"data_path\"])\n",
        "            # Does nothing else (done)\n",
        "            return\n",
        "\n",
        "        # Reverting back to general purpose HF dataset / tokenizer handling\n",
        "        #--------------------------------\n",
        "\n",
        "        load_dataset_params = {\n",
        "            'path': kargs[\"source\"],\n",
        "            'num_proc': num_cpus\n",
        "        }\n",
        "\n",
        "        # Handle advance params (if set)\n",
        "        if kargs[\"source_data_dir\"] is not None:\n",
        "            load_dataset_params['data_dir'] = kargs[\"source_data_dir\"]\n",
        "        if kargs[\"source_dataset_params\"] is not None:\n",
        "            source_dataset_params = kargs[\"source_dataset_params\"]\n",
        "            for k, v in source_dataset_params.items():\n",
        "                load_dataset_params[k] = v\n",
        "\n",
        "        # Load the dataset\n",
        "        src_dataset = load_dataset(**load_dataset_params)\n",
        "\n",
        "        # If for some reason the dataset is a \"test\" only split, and missing a \"train\" split, we remap it as a \"train\" split\n",
        "        if \"train\" not in src_dataset.keys():\n",
        "            if \"test\" in src_dataset.keys():\n",
        "                src_dataset[\"train\"] = src_dataset[\"test\"]\n",
        "                del src_dataset[\"test\"]\n",
        "            else:\n",
        "                raise ValueError('Dataset must have a \"train\" split')\n",
        "\n",
        "        # If an int value is used, it is interprated as document count\n",
        "        # If a floating value (<1.0) is used, it is interprated as a percentage of the dataset\n",
        "        if kargs[\"dataset_offset\"] > 0 or kargs[\"dataset_length\"] > 0:\n",
        "            # src dataset length\n",
        "            train_length = len(src_dataset[\"train\"])\n",
        "\n",
        "            # Compute the offset position\n",
        "            offset_val = kargs[\"dataset_offset\"]\n",
        "\n",
        "            # If offset is a float, we will use it as a percentage\n",
        "            if offset_val < 0:\n",
        "                offset_val = 0\n",
        "            if offset_val > 0 and offset_val < 1.0:\n",
        "                offset_val = int(train_length * offset_val) # Rounded down value\n",
        "\n",
        "            # Compute the length position\n",
        "            length_val = kargs[\"dataset_length\"]\n",
        "            if length_val < 0:\n",
        "                length_val = train_length - offset_val\n",
        "            if length_val > 0 and length_val < 1.0:\n",
        "                length_val = int(train_length * length_val)\n",
        "            if length_val > (train_length - offset_val):\n",
        "                length_val = (train_length - offset_val)\n",
        "\n",
        "            # Get the subset of the dataset\n",
        "            src_dataset[\"train\"] = src_dataset[\"train\"].select(range(offset_val, offset_val + length_val))\n",
        "\n",
        "        # Tokenizer vars\n",
        "        hf_tokenizer = None\n",
        "        world_tokenizer = None\n",
        "\n",
        "        # Load the tokenizer according to either its predefined name or its path\n",
        "        # (defaults to neox)\n",
        "        if kargs[\"tokenizer\"] == \"neox\":\n",
        "            tokenizer_file = os.path.join(SRC_DIR, \"./dataflow/20B_tokenizer.json\")\n",
        "            hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
        "        elif kargs[\"tokenizer\"] == \"world\":\n",
        "            # Setup the tokenizer\n",
        "            world_tokenizer = True\n",
        "        else:\n",
        "            # AutoTokenizer\n",
        "            tokenizerName = kargs[\"tokenizer\"]\n",
        "\n",
        "            # with custom args and props\n",
        "            tokenizerKWArgs = {}\n",
        "            tokenizerProps = {}\n",
        "            if kargs[\"autoTokenizer\"] is not None:\n",
        "                if kargs[\"autoTokenizer\"][\"kwargs\"] is not None:\n",
        "                    tokenizerKWArgs = kargs[\"autoTokenizer\"][\"kwargs\"]\n",
        "                if kargs[\"autoTokenizer\"][\"props\"] is not None:\n",
        "                    tokenizerProps  = kargs[\"autoTokenizer\"][\"props\"]\n",
        "\n",
        "            # Intialize the tokenizer, with kwargs\n",
        "            hf_tokenizer = AutoTokenizer.from_pretrained(tokenizerName, **tokenizerKWArgs)\n",
        "\n",
        "            # Configure the tokenizer properties\n",
        "            for k, v in tokenizerProps.items():\n",
        "                setattr(hf_tokenizer, k, v)\n",
        "\n",
        "        # Function used to tokenize the dataset as per HF tokenizer format\n",
        "        # if given the textual data, it will return the tokenized data\n",
        "        def encodeTokens(x):\n",
        "            if world_tokenizer is True:\n",
        "                # If x is an array of strings, we encode them seperately, and conslidate the result\n",
        "                if isinstance(x, list):\n",
        "                    id_arr = []\n",
        "                    type_arr = []\n",
        "                    mask_arr = []\n",
        "                    for i in range(len(x)):\n",
        "                        enc_str = world_tokenizer_encode(x[i], world_add_endoftext_token=world_add_endoftext_token)\n",
        "                        id_arr.append(enc_str)\n",
        "                        type_arr.append([0] * len(enc_str))\n",
        "                        mask_arr.append([1] * len(enc_str))\n",
        "\n",
        "                    # Consolidate the result\n",
        "                    return {\n",
        "                        'input_ids': id_arr,\n",
        "                        'token_type_ids': type_arr,\n",
        "                        'attention_mask': mask_arr\n",
        "                    }\n",
        "\n",
        "                # Else we encode the string and return it following the HF tokenizer format\n",
        "                enc_str = world_tokenizer_encode(x, world_add_endoftext_token=world_add_endoftext_token)\n",
        "                return {\n",
        "                    'input_ids': enc_str,\n",
        "                    'token_type_ids': [0] * len(enc_str),\n",
        "                    'attention_mask': [1] * len(enc_str)\n",
        "                }\n",
        "\n",
        "            # We use the HF tokenizer as it is, and get the input_ids\n",
        "            return hf_tokenizer(x)\n",
        "\n",
        "        # Multi column merging default values setup\n",
        "        if kargs[\"multi_column_keys\"] is None:\n",
        "            multi_column_keys = ['instruction', 'input', 'output']\n",
        "            multi_column_prefix = ['Instruction:\\n', 'Input:\\n', 'Output:\\n']\n",
        "            multi_column_suffix = ['', '', '']\n",
        "            multi_column_train_mask = [True, False, True]\n",
        "            multi_column_separator = '\\n\\n'\n",
        "        else:\n",
        "            multi_column_keys = kargs[\"multi_column_keys\"]\n",
        "            multi_column_prefix = kargs[\"multi_column_prefix\"]\n",
        "            multi_column_suffix = kargs[\"multi_column_suffix\"]\n",
        "            multi_column_train_mask = kargs[\"multi_column_train_mask\"]\n",
        "            multi_column_separator = kargs[\"multi_column_separator\"]\n",
        "\n",
        "        # Tokenized encodings for multi column keys\n",
        "        multi_column_enabled = len(multi_column_keys) > 0\n",
        "        multi_column_prefix_encodings = []\n",
        "        multi_column_suffix_encodings = []\n",
        "        multi_column_separator_encodings = None\n",
        "\n",
        "        # Process the multi column settings\n",
        "        if multi_column_enabled:\n",
        "\n",
        "            # Tokenize the multi column strings\n",
        "            for i in range(len(multi_column_keys)):\n",
        "                if multi_column_prefix is not None and multi_column_prefix[i] is not None:\n",
        "                    multi_column_prefix_encodings.append(encodeTokens(multi_column_prefix[i]))\n",
        "                if multi_column_suffix is not None and multi_column_suffix[i] is not None:\n",
        "                    multi_column_suffix_encodings.append(encodeTokens(multi_column_suffix[i]))\n",
        "\n",
        "            # Tokenize the multi column separator\n",
        "            if multi_column_separator is not None and len(multi_column_separator) > 0:\n",
        "                multi_column_separator_encodings = encodeTokens(multi_column_separator)\n",
        "\n",
        "        # Maps the dataset record to the tokenized result\n",
        "        # handles a wide variety of format according to the data configuration\n",
        "        #\n",
        "        # - custom text keys\n",
        "        # - multiple key columns merged\n",
        "        # - prompt/completion format\n",
        "        # - text column itself\n",
        "        #\n",
        "        # Throws an error, if it failed to process the record\n",
        "        #\n",
        "        # This is called for each row record in the dataset\n",
        "        def map_tokenizer(x):\n",
        "            # Custom text column support\n",
        "            if kargs[\"custom_text_key\"] is not None:\n",
        "                if kargs[\"custom_text_key\"] in x:\n",
        "                    return encodeTokens(x[kargs[\"custom_text_key\"]])\n",
        "\n",
        "            # Multi column merging support\n",
        "            if multi_column_enabled:\n",
        "                # Lets count the number of columns we have\n",
        "                # that have data in them\n",
        "                num_columns = 0\n",
        "                for i in range(len(multi_column_keys)):\n",
        "                    if multi_column_keys[i] in x and x[multi_column_keys[i]] is not None and len(x[multi_column_keys[i]]) > 0:\n",
        "                        num_columns += 1\n",
        "                # If we have more than 1 column, we will have to merge them\n",
        "                if num_columns > 1:\n",
        "                    # Array of output values we will return\n",
        "                    input_ids = []\n",
        "                    token_type_ids = []\n",
        "                    attention_mask = []\n",
        "\n",
        "                    # First item flag\n",
        "                    is_first_item = True\n",
        "\n",
        "                    # Lets loop through each column\n",
        "                    for i in range(len(multi_column_keys)):\n",
        "                        # And process the column if it has data\n",
        "                        if multi_column_keys[i] in x and x[multi_column_keys[i]] is not None and len(x[multi_column_keys[i]]) > 0:\n",
        "                            # Add the separator if this is not the first item\n",
        "                            if not is_first_item and multi_column_separator_encodings is not None:\n",
        "                                input_ids += multi_column_separator_encodings['input_ids']\n",
        "                                token_type_ids += multi_column_separator_encodings['token_type_ids']\n",
        "                                attention_mask += multi_column_separator_encodings['attention_mask']\n",
        "\n",
        "                            # Add the prefix\n",
        "                            if multi_column_prefix_encodings[i] is not None:\n",
        "                                input_ids += multi_column_prefix_encodings[i]['input_ids']\n",
        "                                token_type_ids += multi_column_prefix_encodings[i]['token_type_ids']\n",
        "                                attention_mask += multi_column_prefix_encodings[i]['attention_mask']\n",
        "\n",
        "                            # Tokenize the column\n",
        "                            column_encodings = encodeTokens(x[multi_column_keys[i]])\n",
        "\n",
        "                            # Add the column\n",
        "                            input_ids += column_encodings['input_ids']\n",
        "                            token_type_ids += column_encodings['token_type_ids']\n",
        "\n",
        "                            # Override the training attention mask if masking is set to false\n",
        "                            if len(multi_column_train_mask) < i and multi_column_train_mask[i] is False:\n",
        "                                attention_mask += ([0] * len(column_encodings['input_ids']))\n",
        "                            else:\n",
        "                                attention_mask += ([1] * len(column_encodings['input_ids']))\n",
        "\n",
        "                            # Add the suffix\n",
        "                            if multi_column_suffix_encodings[i] is not None:\n",
        "                                input_ids += multi_column_suffix_encodings[i]['input_ids']\n",
        "                                token_type_ids += multi_column_suffix_encodings[i]['token_type_ids']\n",
        "                                attention_mask += multi_column_suffix_encodings[i]['attention_mask']\n",
        "\n",
        "                            # Set the first item flag to false\n",
        "                            is_first_item = False\n",
        "\n",
        "                    # Return the merged columns\n",
        "                    return {\n",
        "                        'input_ids': input_ids,\n",
        "                        'token_type_ids': token_type_ids,\n",
        "                        'attention_mask': attention_mask\n",
        "                    }\n",
        "\n",
        "            # Prompt completion support\n",
        "            if 'prompt' in x and 'completion' in x:\n",
        "                # Array of output values we will return\n",
        "                input_ids = None\n",
        "                token_type_ids = None\n",
        "                attention_mask = None\n",
        "\n",
        "                # Tokenize both prompt and completion\n",
        "                # Note that the tokenizer will process and return the input_ids in batches\n",
        "                prompt_encodings = encodeTokens(x['prompt'])\n",
        "                completion_encodings = encodeTokens(x['completion'])\n",
        "\n",
        "                # Join the two input_ids lists\n",
        "                input_ids = prompt_encodings['input_ids'] + completion_encodings['input_ids']\n",
        "                # Join the two token_type_ids lists\n",
        "                token_type_ids = prompt_encodings['token_type_ids'] + completion_encodings['token_type_ids']\n",
        "                # Setup the attention mask, 0 for prompt, 1 for completion, if masking is enabled\n",
        "                if kargs[\"disable_prompt_completion_mask\"]:\n",
        "                    attention_mask = ([1] * len(prompt_encodings['input_ids']) + [1] * len(completion_encodings['input_ids']))\n",
        "                else:\n",
        "                    attention_mask = ([0] * len(prompt_encodings['input_ids']) + [1] * len(completion_encodings['input_ids']))\n",
        "\n",
        "                # Prepare and return the output object\n",
        "                return {\n",
        "                    'input_ids': input_ids,\n",
        "                    'token_type_ids': token_type_ids,\n",
        "                    'attention_mask': attention_mask,\n",
        "                }\n",
        "\n",
        "            # Fallback to standard text tokenization\n",
        "            if 'text' in x:\n",
        "                return encodeTokens(x['text'])\n",
        "\n",
        "            raise ValueError('Invalid dataset format, must contain either the configured \"multi column\" or prompt/completion or text')\n",
        "\n",
        "        # Map the dataset to the tokenizer, removing the old text column\n",
        "        src_dataset = src_dataset.map(map_tokenizer, batched=False, num_proc=num_cpus)\n",
        "\n",
        "        # Remove all features, except input_ids, token_type_ids and attention_mask\n",
        "        # as the metadata/etc columns may cause problems down the line (when passed to the trainer)\n",
        "        dataset_features = src_dataset[\"train\"].features\n",
        "        dataset_features_to_remove = {k: v for k, v in dataset_features.items() if k not in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
        "        src_dataset = src_dataset.remove_columns(list(dataset_features_to_remove.keys()))\n",
        "\n",
        "        # Get the newline token\n",
        "        newline_tokenSet = encodeTokens([\"\\n\"])\n",
        "\n",
        "        # See if rechunking is needed, this is useful mostly for \"text\" based datasets\n",
        "        # where we would need to split them into \"digestable\" context length sizes\n",
        "        # used for foundation training\n",
        "        # ---\n",
        "\n",
        "        # The rechunking function\n",
        "        def rechunk_text(x):\n",
        "            # Full Raw values that we will need to \"rechunk\"\n",
        "            full_input_ids = []\n",
        "            full_token_type_ids = []\n",
        "            full_attention_mask = []\n",
        "\n",
        "            # Loop through the x input, and build the raw values\n",
        "            for i in range(len(x[\"input_ids\"])):\n",
        "                # Get the respective values and push them to the\n",
        "                # raw value array, effectively merging the arrays together\n",
        "                # with the newline token in between\n",
        "                full_input_ids += x[\"input_ids\"][i] + newline_tokenSet[\"input_ids\"][0]\n",
        "                full_token_type_ids += x[\"token_type_ids\"][i] + newline_tokenSet[\"token_type_ids\"][0]\n",
        "                full_attention_mask += x[\"attention_mask\"][i] + newline_tokenSet[\"attention_mask\"][0]\n",
        "\n",
        "            # Total length, and sample count\n",
        "            # note that thte \"remainder\" will be discarded\n",
        "            total_len = len(full_input_ids)\n",
        "            total_samples = total_len // kargs[\"text_rechunk_size\"]\n",
        "\n",
        "            # The output arrays\n",
        "            out_input_ids = []\n",
        "            out_token_type_ids = []\n",
        "            out_attention_mask = []\n",
        "\n",
        "            # Generate the output arrays\n",
        "            for i in range(total_samples):\n",
        "                # Calculate the start and end of the sample\n",
        "                start = i * kargs[\"text_rechunk_size\"]\n",
        "                end = start + kargs[\"text_rechunk_size\"]\n",
        "\n",
        "                # Push the sample to the output arrays\n",
        "                out_input_ids.append(full_input_ids[start:end])\n",
        "                out_token_type_ids.append(full_token_type_ids[start:end])\n",
        "                out_attention_mask.append(full_attention_mask[start:end])\n",
        "\n",
        "            # Prepare and return the output object\n",
        "            ret = {\n",
        "                'input_ids': out_input_ids,\n",
        "                'token_type_ids': out_token_type_ids,\n",
        "                'attention_mask': out_attention_mask,\n",
        "            }\n",
        "            return ret\n",
        "\n",
        "        # Perform rechunking if needed for \"text\" based datasets\n",
        "        if kargs[\"source\"] == \"text\" and kargs[\"text_rechunk_size\"] > 0:\n",
        "            src_dataset = src_dataset.map(rechunk_text, batched=True,\n",
        "                                        batch_size=kargs[\"text_rechunk_size\"]*10,\n",
        "                                        num_proc=num_cpus)\n",
        "\n",
        "        # Remove empty datasets (it causes an error otherwise)\n",
        "        # and perform min/max length filtering (if configured)\n",
        "        def dataset_filter(x):\n",
        "            row_length = len(x[\"input_ids\"])\n",
        "            if row_length <= 0:\n",
        "                return False\n",
        "            if kargs[\"min_token_size\"] > 0 and row_length < kargs[\"min_token_size\"]:\n",
        "                return False\n",
        "            if kargs[\"max_token_size\"] > 0 and row_length > kargs[\"max_token_size\"]:\n",
        "                return False\n",
        "            return True\n",
        "        src_dataset = src_dataset.filter(dataset_filter, num_proc=num_cpus)\n",
        "\n",
        "        # Perform a sort by length\n",
        "        if kargs[\"sort_by_length\"]:\n",
        "            sort_asc = kargs[\"sort_asc\"]\n",
        "\n",
        "            def add_length(example):\n",
        "                example[\"length\"] = len(example['input_ids'])\n",
        "                return example\n",
        "\n",
        "            src_dataset = src_dataset.map(add_length)\n",
        "\n",
        "            # sort by length (not sorting the columns, just the rows)\n",
        "            src_dataset = src_dataset.sort(\"length\", reverse=not sort_asc)\n",
        "\n",
        "        # Perform rechunking after filtering, if source is not a \"text\" based\n",
        "        # dataset and text_rechunk_force is enabled\n",
        "        if kargs[\"source\"] != \"text\" and kargs[\"text_rechunk_size\"] > 0 and kargs[\"text_rechunk_force\"]:\n",
        "            src_dataset = src_dataset.map(rechunk_text, batched=True,\n",
        "                                        batch_size=kargs[\"text_rechunk_size\"]*2,\n",
        "                                        num_proc=num_cpus)\n",
        "\n",
        "        # Check if the dataset does not have a test split\n",
        "        # and if so, perform the split\n",
        "        if 'test' not in src_dataset.keys():\n",
        "            test_split = kargs[\"test_split\"]\n",
        "            # The minimum test size is 1, if not we will get errors in the trainer?\n",
        "            if test_split <= 0 or test_split <= 0.0:\n",
        "                test_split = 1\n",
        "            src_dataset = src_dataset['train'].train_test_split(\n",
        "                test_size=test_split,shuffle=kargs[\"test_split_shuffle\"],\n",
        "                seed=42 #Fixed seed, to prevent train/test reshuffling between test runs\n",
        "            )\n",
        "\n",
        "        # Save the dataset to disk\n",
        "        src_dataset.save_to_disk(kargs[\"data_path\"])\n",
        "\n",
        "\n",
        "class RWKVDataModule(LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        # load_from_disk(dataset_path) param\n",
        "        data_path: str,\n",
        "        # load_dataset(path) param\n",
        "        source: str = None,\n",
        "        # load_dataset(data_dir) param\n",
        "        source_data_dir: str = None,\n",
        "        # Additional dataset params\n",
        "        source_dataset_params: dict = None,\n",
        "        # Test split of source data, if it was not already done\n",
        "        test_split: float = 0.01,\n",
        "        test_split_shuffle: bool = False,\n",
        "        # Text rechunking size\n",
        "        text_rechunk_size: int = 4096,\n",
        "        text_rechunk_force: bool = False,\n",
        "        # ---\n",
        "        # Tokenizer settings\n",
        "        # ---\n",
        "        tokenizer: str = \"neox\",\n",
        "        autoTokenizer = None,\n",
        "\n",
        "        # Add <|endoftext|> string token to the world tokenizer, at index 0\n",
        "        # this was missing from the original world trie_tokenizer\n",
        "        world_add_endoftext_token: bool = True,\n",
        "\n",
        "        # ---\n",
        "        # HF dataset conversion helpers\n",
        "        # ---\n",
        "        # Min / Max token size filtering\n",
        "        min_token_size: int = -1,\n",
        "        max_token_size: int = -1,\n",
        "\n",
        "        # Sort by length\n",
        "        sort_by_length: bool = False,\n",
        "        sort_asc: bool = True,\n",
        "\n",
        "        # Dataset offset and limit controls\n",
        "        dataset_offset: int = -1,\n",
        "        dataset_length: int = -1,\n",
        "\n",
        "        # Custom 'text' column to support, mostly used for dataset where the\n",
        "        # desired train data is in another column (eg. 'code')\n",
        "        custom_text_key: str = None,\n",
        "        # Multi column merging support, used for instruct/input/output datasets\n",
        "        # or similar varients where the input and output are in different columns\n",
        "        # and need to be merged\n",
        "        multi_column_keys: list = None,\n",
        "        multi_column_prefix: list = None,\n",
        "        multi_column_suffix: list = None,\n",
        "        multi_column_train_mask: list = None,\n",
        "        multi_column_separator: str = None,\n",
        "        # prompt/completion format masking support\n",
        "        disable_prompt_completion_mask: bool = False,\n",
        "        # Skip database setup checks if datapath exists, ignored if using preload_datapath.py\n",
        "        skip_datapath_setup: bool = False,\n",
        "\n",
        "        multitoken_width: int = 1,\n",
        "    ):\n",
        "        # Capture the init parameters\n",
        "        self._init_locals = locals()\n",
        "        del self._init_locals[\"self\"]\n",
        "        del self._init_locals[\"__class__\"]\n",
        "\n",
        "        super().__init__()\n",
        "        self.data_path = data_path\n",
        "        self._loaded_dataset = None\n",
        "\n",
        "        # Log to wandb\n",
        "        if wandb.run is not None:\n",
        "            wandb.config.update({ \"data\":dict(self._init_locals) })\n",
        "\n",
        "    # Called once for initial setup\n",
        "    def prepare_data(self):\n",
        "        prepare_data_static(**self._init_locals)\n",
        "\n",
        "    # Setup process that is universal\n",
        "    def _internal_setup(self):\n",
        "        if self._loaded_dataset is None:\n",
        "            self._loaded_dataset = load_from_disk(self.data_path).with_format('torch')\n",
        "\n",
        "    # Called once for every process in DDP\n",
        "    def setup(self, stage):\n",
        "        self._internal_setup()\n",
        "\n",
        "    # Return the train dataloader\n",
        "    def train_dataloader(self):\n",
        "        self._internal_setup()\n",
        "        return DataLoader(self._loaded_dataset['train'], num_workers=num_workers)\n",
        "\n",
        "    # Return the validation dataloader\n",
        "    def val_dataloader(self):\n",
        "        self._internal_setup()\n",
        "        return DataLoader(self._loaded_dataset['test'], num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwCXf1rIXIpp",
        "outputId": "6d116846-d55d-4cc9-93ac-33393c741ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/RWKV-infctx-trainer/RWKV-v5/src/model.py\n"
          ]
        }
      ],
      "source": [
        "#@title patch model.py\n",
        "\n",
        "%%writefile /content/RWKV-infctx-trainer/RWKV-v5/src/model.py\n",
        "\n",
        "########################################################################################################\n",
        "# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n",
        "########################################################################################################\n",
        "\n",
        "import gc, math, os\n",
        "from random import randint\n",
        "from typing import List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "# torch._C._jit_set_profiling_executor(True)\n",
        "# torch._C._jit_set_profiling_mode(True)\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import lightning as L\n",
        "from lightning.pytorch.utilities import rank_zero_info, rank_zero_only\n",
        "from lightning.pytorch.strategies import DeepSpeedStrategy\n",
        "\n",
        "import deepspeed\n",
        "from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n",
        "import deepspeed.runtime.lr_schedules\n",
        "import wandb\n",
        "\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "# Script dir for various files\n",
        "SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
        "CUDA_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, \"../cuda\"))\n",
        "\n",
        "# janky hack mate\n",
        "MULTITOKEN_RANGES = tuple(\n",
        "    (i*1024, (i+1)*1024) for i in range(37)\n",
        ")\n",
        "PADDING_IDX = 2**16-1\n",
        "\n",
        "########################################################################################################\n",
        "# JIT / torch compile special handling\n",
        "########################################################################################################\n",
        "\n",
        "# Currently the features we need for torch compile, is avaliable only in\n",
        "# 2.1 nightly build (and is expected to be in 2.1 official release)\n",
        "#\n",
        "# However because the nightly build torch.compile behaviour has been unstable\n",
        "# the versioning code to check for enabling toch compile will not be used,\n",
        "# until we confirm a stable version of torch.compile\n",
        "from packaging import version\n",
        "def is_torch_version_above(required_version):\n",
        "    torch_version = version.parse(torch.__version__.split('+')[0])\n",
        "    return torch_version >= version.parse(required_version)\n",
        "IS_TORCH_2_1 = is_torch_version_above(\"2.0.9999\")\n",
        "\n",
        "# Get the JIT / torch compile option flags from the environment\n",
        "RWKV_JIT_ON        = os.getenv(\"RWKV_JIT_ON\", \"1\").lower() in (\"1\", \"true\", \"yes\")\n",
        "RWKV_TORCH_COMPILE = os.getenv(\"RWKV_TORCH_COMPILE\", f\"0\").lower() in (\"1\", \"true\", \"yes\")\n",
        "RWKV_TORCH_RUN_MODE = None\n",
        "\n",
        "# We enable JITMod*/Function when supporting torch.jit\n",
        "# We use TorchCompile* when supporting torch compile\n",
        "# based on the current runtime settings\n",
        "if RWKV_TORCH_COMPILE:\n",
        "    RWKV_TORCH_RUN_MODE = \"torch-compile\"\n",
        "\n",
        "    JITModClass  = nn.Module\n",
        "    JITModMethod = lambda x: x\n",
        "    JITFunction  = lambda x: x\n",
        "\n",
        "    # PS: i have tried mode=\"max-autotune\", and mode=\"reduce-overhead\", however they crash\n",
        "    #     for now (8th July 2023). I may introduce them in the future once they are stable\n",
        "    #\n",
        "    #     Additionally, torch.compile has issues with the pytorch.lightning module directly\n",
        "    # ---\n",
        "\n",
        "    # We generally have 2 major options, either we use torch.compile\n",
        "    # onto the key top level functions (train, val, test, predict, etc)\n",
        "    # and let the compiler handle all the decision making on how to optimize\n",
        "    #\n",
        "    # However this was found to basically just match JIT level of performance exactly\n",
        "    # ---\n",
        "    # TCompileMax          = lambda x: x\n",
        "    # TCompileBaseline     = lambda x: torch.compile(x, fullgraph=False)\n",
        "\n",
        "    # Alternatively, we can perform a much more aggressive optimization on critical functions\n",
        "    # that we know are compatible with torch.compile(fullgraph=True) - which provides the highest\n",
        "    # level of optimization possible with torch.compile\n",
        "    # ---\n",
        "    TCompileMax        = lambda x: torch.compile(x, fullgraph=True)\n",
        "    TCompileBaseline   = lambda x: x\n",
        "\n",
        "    # ---\n",
        "    # Because torch.compile is expected to change overtime, the two options should\n",
        "    # be tested every now and then, for any performance changes\n",
        "    #\n",
        "    # and we should switch over to the broaded automated approach if its \"faster\"\n",
        "    # ---\n",
        "\n",
        "    # Used to wrap functions which are **not** torch.compile compatible\n",
        "    TCompileDisable    = torch._dynamo.disable\n",
        "\n",
        "    # The following are known warnings in the nightly build, that can be safely ignored for stable release\n",
        "    #\n",
        "    # `torch._inductor.utils: [WARNING] DeviceCopy in input program`\n",
        "    # https://discuss.pytorch.org/t/what-can-cause-warning-devicecopy-in-input-program/175566\n",
        "\n",
        "elif RWKV_JIT_ON:\n",
        "    RWKV_TORCH_RUN_MODE = \"torch-jit\"\n",
        "    JITModClass  = torch.jit.ScriptModule\n",
        "    JITModMethod = torch.jit.script_method\n",
        "    JITFunction  = torch.jit.script\n",
        "\n",
        "    # JITModClass  = nn.Module\n",
        "    # JITModMethod = lambda x: x\n",
        "    # JITFunction  = lambda x: x\n",
        "\n",
        "    TCompileMax        = lambda x: x\n",
        "    TCompileBaseline   = lambda x: x\n",
        "    TCompileDisable    = lambda x: x\n",
        "else:\n",
        "    RWKV_TORCH_RUN_MODE = \"torch-native\"\n",
        "    JITModClass  = nn.Module\n",
        "    JITModMethod = lambda x: x\n",
        "    JITFunction  = lambda x: x\n",
        "\n",
        "    TCompileMax        = lambda x: x\n",
        "    TCompileBaseline   = lambda x: x\n",
        "    TCompileDisable    = lambda x: x\n",
        "\n",
        "print(f\"[RWKV.model] Running RWKV model using '{RWKV_TORCH_RUN_MODE}' with torch '{torch.__version__}'\")\n",
        "\n",
        "# ---\n",
        "# Isolating out known operations that **does not work** with torch.compile\n",
        "# and wrapping them within a torch._dynamo.disable, this is required to get\n",
        "# the baseline torc.compile to work\n",
        "# ---\n",
        "\n",
        "@TCompileDisable\n",
        "def deepspeed_checkpoint(*args, **kwargs):\n",
        "    return deepspeed.checkpointing.checkpoint(*args, **kwargs)\n",
        "\n",
        "########################################################################################################\n",
        "# RWKV: State Blocks\n",
        "########################################################################################################\n",
        "\n",
        "class TimeMixState:\n",
        "\n",
        "    def __init__(self, shift_state: torch.Tensor, wkv_state: torch.Tensor):\n",
        "        self.shift_state = shift_state\n",
        "        self.wkv_state = wkv_state\n",
        "\n",
        "\n",
        "class ChannelMixState:\n",
        "\n",
        "    def __init__(self, shift_state: torch.Tensor):\n",
        "        self.shift_state = shift_state\n",
        "\n",
        "\n",
        "class BlockState:\n",
        "\n",
        "    def __init__(self, time_mix_state: TimeMixState,\n",
        "                 channel_mix_state: ChannelMixState):\n",
        "        self.time_mix_state = time_mix_state\n",
        "        self.channel_mix_state = channel_mix_state\n",
        "\n",
        "\n",
        "class BlockStateList:\n",
        "\n",
        "    def __init__(self, shift_states, wkv_states):\n",
        "        self.wkv_states = wkv_states\n",
        "        self.shift_states = shift_states\n",
        "\n",
        "    # @ TCompileMax (no difference)\n",
        "    @staticmethod\n",
        "    def create(N, B, C, n_head, head_size, device, dtype):\n",
        "        result = BlockStateList.empty(N, B, C, n_head, head_size, device, dtype)\n",
        "        result.wkv_states[:] = 0\n",
        "        # result.wkv_states[:, :, :, -1] = -1e38\n",
        "        result.shift_states[:] = 0\n",
        "        return result\n",
        "\n",
        "    # @ TCompileMax (no difference)\n",
        "    @staticmethod\n",
        "    def empty(N, B, C, n_head, head_size, device, dtype):\n",
        "        # @TODO: confirm if dtype can be changed from .flaot to dtype=dtype (when bf16)\n",
        "        wkv_states = torch.empty((N, B, n_head, head_size, head_size),\n",
        "                                 device=device,\n",
        "                                #  dtype=dtype)\n",
        "                                 dtype=torch.float)\n",
        "        shift_states = torch.empty((N, 2, B, C), device=device, dtype=dtype)\n",
        "        return BlockStateList(shift_states, wkv_states)\n",
        "\n",
        "    def __getitem__(self, layer: int):\n",
        "        return BlockState(\n",
        "            TimeMixState(self.shift_states[layer, 0], self.wkv_states[layer]),\n",
        "            ChannelMixState(self.shift_states[layer, 1]))\n",
        "\n",
        "    def __setitem__(self, layer: int, state: BlockState):\n",
        "        self.shift_states[layer, 0] = state.time_mix_state.shift_state\n",
        "        self.wkv_states[layer] = state.time_mix_state.wkv_state\n",
        "        self.shift_states[layer, 1] = state.channel_mix_state.shift_state\n",
        "\n",
        "########################################################################################################\n",
        "# RWKV: RWKV Time-mix + RWKV Channel-mix\n",
        "########################################################################################################\n",
        "\n",
        "class RWKV_TimeMix(JITModClass):\n",
        "\n",
        "    def __init__(self, layer_id, n_layer, n_embd, n_head, head_size, dim_att):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_att = dim_att\n",
        "        self.n_layer = n_layer\n",
        "        self.n_embd = n_embd\n",
        "        self.layer_id = layer_id\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.head_size = head_size\n",
        "\n",
        "        # Optimized chunk length is fixed for now\n",
        "        self.chunk_len = 512\n",
        "        # assert ctx_len % self.chunk_len == 0\n",
        "\n",
        "        with torch.no_grad():  # fancy init\n",
        "            ratio_0_to_1 = layer_id / (n_layer - 1)  # 0 to 1\n",
        "            ratio_1_to_almost0 = 1.0 - (layer_id / n_layer)  # 1 to ~0\n",
        "            ddd = torch.ones(1, 1, n_embd)\n",
        "            for i in range(n_embd):\n",
        "                ddd[0, 0, i] = i / n_embd\n",
        "\n",
        "            # fancy time_mix\n",
        "            self.time_mix_k = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n",
        "            self.time_mix_v = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n",
        "            self.time_mix_r = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n",
        "\n",
        "            # fancy time_decay\n",
        "            decay_speed = torch.ones(n_head)\n",
        "            for h in range(n_head):\n",
        "                decay_speed[h] = -8 + 7 * (h / (n_head - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n",
        "            self.time_decay = nn.Parameter(decay_speed)\n",
        "            # print(layer_id, self.time_decay.flatten()[:3].cpu().numpy(), '...', self.time_decay.flatten()[-3:].cpu().numpy())\n",
        "\n",
        "            # V5-R2 changes\n",
        "            self.time_faaaa = nn.Parameter(torch.ones(n_head) * 0.05)\n",
        "            # self.time_first = nn.Parameter(torch.ones(n_head) * (-3.0))\n",
        "\n",
        "        # self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
        "        self.receptance = nn.Linear(n_embd, dim_att, bias=False)\n",
        "        self.key = nn.Linear(n_embd, dim_att, bias=False)\n",
        "        self.value = nn.Linear(n_embd, dim_att, bias=False)\n",
        "        self.output = nn.Linear(dim_att, n_embd, bias=False)\n",
        "\n",
        "        self.ln_x = nn.GroupNorm(n_head, n_embd)\n",
        "\n",
        "    # this is based on jit_func(self,x)\n",
        "    @JITModMethod\n",
        "    def _forward_rkv_chunk(self, x, B, TT, last_state: TimeMixState):\n",
        "        # Mix x with the previous timestep to produce xk, xv, xr\n",
        "        xx = torch.concat((last_state.shift_state.unsqueeze(1), x[:, :-1]), dim=1)\n",
        "        # xx = self.time_shift(x)\n",
        "\n",
        "        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n",
        "        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n",
        "        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n",
        "\n",
        "        r = self.receptance(xr).view(B, TT, self.n_head, self.head_size).transpose(1, 2)            # BTC -> BHTS\n",
        "        k = self.key(xk).view(B, TT, self.n_head, self.head_size).transpose(1, 2).transpose(-2, -1) # BTC -> BHTS -> BHST\n",
        "        v = self.value(xv).view(B, TT, self.n_head, self.head_size).transpose(1, 2)                 # BTC -> BHTS\n",
        "\n",
        "        return r, k, v\n",
        "\n",
        "    def _forward_wkbs_chunk(self, T, r, k, v):\n",
        "        H = self.n_head\n",
        "\n",
        "        w = torch.exp(-torch.exp(self.time_decay.float())).unsqueeze(-1)\n",
        "\n",
        "        # V5-R2 changes\n",
        "        u = self.time_faaaa.float().unsqueeze(-1)\n",
        "        # u = torch.exp(self.time_first.float()).unsqueeze(-1)\n",
        "\n",
        "        ws = w.pow(T).reshape(1, H, 1, 1)\n",
        "        ind = torch.arange(T-1, -1, -1, device=r.device).unsqueeze(0).repeat(H, 1)\n",
        "        w = w.repeat(1, T).pow(ind)\n",
        "\n",
        "        wk = w.reshape(1, H, 1, T)\n",
        "        wb = wk.transpose(-2, -1).flip(2)\n",
        "\n",
        "        w = torch.cat([w[:, 1:], u], dim=1)\n",
        "        w = F.pad(w, (0, T))\n",
        "        w = torch.tile(w, [T])\n",
        "        w = w[:, :-T].reshape(-1, T, 2 * T - 1)\n",
        "        w = w[:, :, T-1:].reshape(1, H, T, T)\n",
        "\n",
        "        w = w.to(dtype=r.dtype)\n",
        "        wk = wk.to(dtype=r.dtype)\n",
        "        wb = wb.to(dtype=r.dtype)\n",
        "        ws = ws.to(dtype=r.dtype)\n",
        "\n",
        "        return w, wk, wb, ws\n",
        "\n",
        "    @JITModMethod\n",
        "    def _forward_state_chunk(self, r, k, v, w, wk, wb, ws, x_l, last_state: TimeMixState):\n",
        "        B, H, TT, S = r.size()\n",
        "        T = TT\n",
        "\n",
        "        # s = torch.zeros(B, H, S, S, device=r.device, dtype=r.dtype)  # state\n",
        "        s = last_state.wkv_state\n",
        "\n",
        "        if r.dtype == torch.bfloat16 and s.dtype != torch.bfloat16:\n",
        "            s = s.contiguous().to(torch.bfloat16)\n",
        "\n",
        "        x = torch.zeros(B, H, TT, S, device=r.device, dtype=r.dtype) # output\n",
        "\n",
        "        ########################################################################\n",
        "        for i in range(TT // T):\n",
        "\n",
        "            rr = r[:, :, i*T:i*T+T, :]\n",
        "            kk = k[:, :, :, i*T:i*T+T]\n",
        "            vv = v[:, :, i*T:i*T+T, :]\n",
        "\n",
        "            x[:, :, i*T:i*T+T, :] = ((rr @ kk) * w) @ vv  +  (rr @ s) * wb\n",
        "\n",
        "            s = ws * s + (kk * wk) @ vv\n",
        "        ########################################################################\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(B * TT, H*S) # BHTS -> BTHS -> BTC\n",
        "        x = self.ln_x(x/8).view(B, TT, H*S)\n",
        "\n",
        "        return self.output(x), TimeMixState(x_l, s)\n",
        "\n",
        "    def _forward_chunk(self, x, last_state: TimeMixState):\n",
        "        # Forward sizings (Batch, Time/ContextLength, Tokens)\n",
        "        B, TT, C = x.size()\n",
        "        B = torch.tensor(B, device=x.device, dtype=torch.int32)\n",
        "        TT = torch.tensor(TT, device=x.device, dtype=torch.int32)\n",
        "\n",
        "        # Get r, k, v (self.jit_func(x))\n",
        "        r, k, v = self._forward_rkv_chunk(x, B, TT, last_state)\n",
        "\n",
        "        # Get w, wk, wb, ws (self.jit_func_2)\n",
        "        w, wk, wb, ws = self._forward_wkbs_chunk(TT, r, k, v)\n",
        "\n",
        "        # Does the state forwarding\n",
        "        return self._forward_state_chunk(r, k, v, w, wk, wb, ws, x[:, -1], last_state)\n",
        "\n",
        "    @TCompileMax\n",
        "    def forward(self, x, last_state: TimeMixState):\n",
        "        # Get the x sizing\n",
        "        B, TT, C = x.size()\n",
        "\n",
        "        # Chunk length to split by,\n",
        "        # we probably can reoptimize this at some point\n",
        "        chunk_len = self.chunk_len\n",
        "\n",
        "        # Logits to return\n",
        "        x_logits = torch.zeros(B, TT, C, device=x.device, dtype=x.dtype)\n",
        "\n",
        "        # Split the input by TT chunks\n",
        "        for i in range(0, TT, chunk_len):\n",
        "            x_chunk = x[:, i:i+chunk_len, :]\n",
        "            chunk_logits, last_state = self._forward_chunk(x_chunk, last_state)\n",
        "            x_logits[:, i:i+chunk_len, :] = chunk_logits\n",
        "\n",
        "        # Return the logits and the state\n",
        "        return x_logits, last_state\n",
        "\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "\n",
        "class RWKV_ChannelMix(JITModClass):\n",
        "\n",
        "    def __init__(self, layer_id, n_layer, n_embd, dim_ffn):\n",
        "        super().__init__()\n",
        "\n",
        "        with torch.no_grad():  # fancy init of time_mix\n",
        "            ratio_1_to_almost0 = 1.0 - (layer_id / n_layer)  # 1 to ~0\n",
        "            ddd = torch.ones(1, 1, n_embd)\n",
        "            for i in range(n_embd):\n",
        "                ddd[0, 0, i] = i / n_embd\n",
        "            self.time_mix_k = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n",
        "            self.time_mix_r = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n",
        "\n",
        "        self.key = nn.Linear(n_embd, dim_ffn, bias=False)\n",
        "        self.receptance = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.value = nn.Linear(dim_ffn, n_embd, bias=False)\n",
        "\n",
        "    @JITModMethod\n",
        "    @TCompileMax\n",
        "    def forward(self, x, last_state: ChannelMixState):\n",
        "        xx = torch.concat((last_state.shift_state.unsqueeze(1), x[:, :-1]),\n",
        "                          dim=1)\n",
        "        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n",
        "        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n",
        "        k = self.key(xk)\n",
        "        k = torch.square(torch.relu(k))\n",
        "        kv = self.value(k)\n",
        "        return (torch.sigmoid(self.receptance(xr)) * kv,\n",
        "                ChannelMixState(x[:, -1]))\n",
        "\n",
        "\n",
        "########################################################################################################\n",
        "# The RWKV Model blocks\n",
        "########################################################################################################\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_id, n_layer, n_embd, n_head, head_size, dropout, dim_att, dim_ffn):\n",
        "        super().__init__()\n",
        "        self.layer_id = layer_id\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        if self.layer_id == 0:\n",
        "            self.ln0 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        self.att = RWKV_TimeMix(layer_id, n_layer, n_embd, n_head, head_size, dim_att)\n",
        "        self.ffn = RWKV_ChannelMix(layer_id, n_layer, n_embd, dim_ffn)\n",
        "\n",
        "        # Setup droupout at block level\n",
        "        self.dropout = dropout\n",
        "        if dropout > 0:\n",
        "            self.drop0 = nn.Dropout(p = dropout)\n",
        "            self.drop1 = nn.Dropout(p = dropout)\n",
        "\n",
        "    def forward(self, x, last_state: BlockState):\n",
        "        if self.layer_id == 0:\n",
        "            x = self.ln0(x)\n",
        "\n",
        "        att_out, att_state = self.att(\n",
        "            self.ln1(x),\n",
        "            last_state.time_mix_state,\n",
        "        )\n",
        "\n",
        "        if self.dropout > 0.0:\n",
        "            # Handle with dropout\n",
        "            x = self.drop0(x + att_out)\n",
        "            ffn_out, ffn_state = self.ffn(\n",
        "                self.ln2(x),\n",
        "                last_state.channel_mix_state,\n",
        "            )\n",
        "            x = self.drop1(x + ffn_out)\n",
        "        else:\n",
        "            # Handle without dropout\n",
        "            x = x + att_out\n",
        "            ffn_out, ffn_state = self.ffn(\n",
        "                self.ln2(x),\n",
        "                last_state.channel_mix_state,\n",
        "            )\n",
        "            x = x + ffn_out\n",
        "\n",
        "        return x, BlockState(att_state, ffn_state)\n",
        "\n",
        "\n",
        "class L2Wrap(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, loss, y, token_amount, currentMask):\n",
        "        # Currently (8th July 2023), save_for_backward, causes an issue with\n",
        "        # pytorch.compile (see: https://github.com/pytorch/pytorch/blob/e600505e3209eaf539e8bc99870ea55236cefbf5/torch/_dynamo/variables/higher_order_ops.py#L735)\n",
        "        #\n",
        "        # Due to L2Wrap being a major hotspot, we should monitor this for future support.\n",
        "        # so that once its resolved, we can include the L2Wrap step in the torch.compile path\n",
        "        #\n",
        "        # See also:\n",
        "        # - checkpointed_step\n",
        "        ctx.save_for_backward(y)\n",
        "        ctx.token_amount = token_amount\n",
        "        ctx.currentMask = currentMask\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        y, = ctx.saved_tensors\n",
        "        token_amount = ctx.token_amount\n",
        "        # to encourage the logits to be close to 0\n",
        "        factor = 1e-4 / token_amount\n",
        "        maxx, ids = torch.max(y, -1, keepdim=True)\n",
        "        gy = torch.zeros_like(y)\n",
        "        gy.scatter_(-1, ids, maxx * factor)\n",
        "        gy = gy * ctx.currentMask[:, None][None, :]\n",
        "        return (grad_output, gy, None, None)\n",
        "\n",
        "########################################################################################################\n",
        "# Static optimized functions\n",
        "########################################################################################################\n",
        "\n",
        "# @ TCompileMax (no speed improvement)\n",
        "# def F_cross_entropy_reduction_none_optimized(logits, targets):\n",
        "#     return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"none\")\n",
        "\n",
        "########################################################################################################\n",
        "# Core RWKV module\n",
        "########################################################################################################\n",
        "class RWKV(L.LightningModule):\n",
        "\n",
        "    def __init__(self,\n",
        "                 # Model file path to load from\n",
        "                 load_model: str,\n",
        "                 # Model size settings, which we either\n",
        "                 # \"auto detect\", or use the user specified settings\n",
        "                 n_embd: int = -1,\n",
        "                 n_layer: int = -1,\n",
        "                 vocab_size: int = -1,\n",
        "                 # Context length size for the model\n",
        "                 ctx_len: int = 2048,\n",
        "                 # Context length schedule\n",
        "                 ctx_len_cutoffs: List[int] = [],\n",
        "                 ctx_len_warmup_steps: List[int] = [],\n",
        "                 # Learning rate schedule\n",
        "                 # use only target_lr_init / lr_init\n",
        "                 # to configure a constant learning rate\n",
        "                 lr_init: float = -1.0,\n",
        "                 lr_final: float = -1.0,\n",
        "                 lr_period: int = -1,\n",
        "                 lr_period_type: str = 'epoch',\n",
        "                 # Dropout rate\n",
        "                 dropout: float = 0.0,\n",
        "                 # Adam optimizer settings\n",
        "                 beta1: float = 0.9,\n",
        "                 beta2: float = 0.99,\n",
        "                 adam_eps: float = 1.0e-08,\n",
        "                 weight_decay: float = 0.01,\n",
        "                 warmup_steps: int = -1,\n",
        "                 # loss bias start\n",
        "                 position_loss_bias: float = 1.0,\n",
        "                 position_loss_bias_in_validation: bool = False,\n",
        "                 # Backprop settings\n",
        "                 grad_cp: bool = True,\n",
        "                 bptt_learning: bool = True,\n",
        "                 bptt_learning_range: int = -1,\n",
        "                 bptt_truncated_learning: bool = False,\n",
        "                 layerwise_lr: bool = True,\n",
        "                 dim_att: Optional[int] = None,\n",
        "                 dim_ffn: Optional[int] = None,\n",
        "                 substep_cuda_cache_clear: bool = False,\n",
        "                 substep_logging: bool = False,\n",
        "                 torch_set_float32_matmul_precision:str = 'high'\n",
        "                 ):\n",
        "\n",
        "        # Lets save everything in one shot\n",
        "        # (this is used for wandb logging)\n",
        "        self.setup_args = locals()\n",
        "        del self.setup_args[\"self\"]\n",
        "        del self.setup_args[\"__class__\"]\n",
        "\n",
        "        # Setup the parent class\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the model, unless its the special \".//<#|=@%!$init_model$!%@=|#>//.\" path\n",
        "        # which is reserved to be used with the `init_model.py`\n",
        "        #\n",
        "        # We intentionally used several filesystem illegal characters, to ensure it\n",
        "        # is not accidentally used by the user for a real file\n",
        "        model_weights = None\n",
        "        model_keys = None\n",
        "        if load_model != \".//<#|=@%!$init_model$!%@=|#>//.\":\n",
        "            # Check if the load_model path exists, and is a file\n",
        "            if not os.path.isfile(load_model):\n",
        "                raise ValueError(f\"load_model file '{load_model}' does not exist\")\n",
        "\n",
        "            # Load the model weights\n",
        "            model_weights = torch.load(load_model, map_location='cpu')\n",
        "\n",
        "            # Get the model keys\n",
        "            model_keys = list(model_weights.keys())\n",
        "\n",
        "        # Lets compute the model various sizes, if they are not provided\n",
        "        if n_layer < 0:\n",
        "            max_block_id = 0\n",
        "            for x in model_keys:\n",
        "                if 'blocks.' in x:\n",
        "                    block_id = int(x.split('.')[1])\n",
        "                    max_block_id = max(max_block_id, block_id)\n",
        "            n_layer = max_block_id + 1\n",
        "\n",
        "        if n_embd < 0:\n",
        "            n_embd = model_weights['head.weight'].shape[1]\n",
        "\n",
        "        if vocab_size < 0:\n",
        "            vocab_size = model_weights['head.weight'].shape[0]\n",
        "\n",
        "        # Save the various other params for later\n",
        "        self.ctx_len = ctx_len\n",
        "        self.ctx_len_cutoffs = ctx_len_cutoffs\n",
        "        self.ctx_len_warmup_steps = ctx_len_warmup_steps\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.layerwise_lr = layerwise_lr\n",
        "        self.grad_cp = grad_cp\n",
        "        self.lr_init = lr_init\n",
        "        self.lr_final = lr_final\n",
        "        self.lr_period = lr_period\n",
        "        self.lr_period_type = lr_period_type\n",
        "        self.dropout = dropout\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.weight_decay = weight_decay\n",
        "        self.adam_eps = adam_eps\n",
        "        self.bptt_learning = bptt_learning\n",
        "        self.bptt_learning_range = bptt_learning_range\n",
        "        self.bptt_truncated_learning = bptt_truncated_learning\n",
        "        self.substep_cuda_cache_clear = substep_cuda_cache_clear\n",
        "        self.substep_logging = substep_logging\n",
        "\n",
        "        # Save the position loss params\n",
        "        self.position_loss_bias = position_loss_bias\n",
        "        self.position_loss_bias_in_validation = position_loss_bias_in_validation\n",
        "\n",
        "        dim_att = dim_att or n_embd\n",
        "        dim_ffn = dim_ffn or n_embd * 4\n",
        "        self.dim_att = dim_att\n",
        "        self.dim_ffn = dim_ffn\n",
        "\n",
        "        # Compute the RWKV-v5 n_head / headsize\n",
        "        head_size = 64\n",
        "        n_head = n_embd // head_size\n",
        "        assert n_embd % n_head == 0 ,  f\"n_embd must be divisible by head_size ({self.head_size})\"\n",
        "        self.n_head = n_head\n",
        "        self.head_size = head_size\n",
        "\n",
        "        # Matmu precision check\n",
        "        if torch_set_float32_matmul_precision is not None:\n",
        "            torch.set_float32_matmul_precision(torch_set_float32_matmul_precision)\n",
        "\n",
        "        self.emb = nn.EmbeddingBag(vocab_size, n_embd, padding_idx=PADDING_IDX)\n",
        "\n",
        "        # load(name=f\"wkv_{self.ctx_len}_bf16\",\n",
        "        #      sources=[\n",
        "        #         os.path.join(CUDA_DIR, \"wkv_op_bf16.cpp\"),\n",
        "        #         os.path.join(CUDA_DIR, \"wkv_cuda_bf16.cu\")\n",
        "        #     ],\n",
        "        #      verbose=True,\n",
        "        #      extra_cflags=[\"-std=c++17\", \"-O3\", f\"-DTmax={self.ctx_len}\"],\n",
        "        #      extra_cuda_cflags=[\n",
        "        #          \"-t 4\", \"-std=c++17\", \"-res-usage\", \"--maxrregcount 60\",\n",
        "        #          \"--use_fast_math\", \"-O3\", \"-Xptxas -O3\",\n",
        "        #          \"--extra-device-vectorization\", f\"-DTmax={self.ctx_len}\"\n",
        "        #      ],\n",
        "        #      is_python_module=False)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(i, n_layer, n_embd, n_head, head_size, dropout, dim_att, dim_ffn) for i in range(n_layer)\n",
        "        ])\n",
        "\n",
        "        self.ln_out = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        # Dropout handling\n",
        "        if dropout > 0:\n",
        "            self.drop0 = nn.Dropout(p = dropout)\n",
        "\n",
        "        # load the state, and GC the original cpu copy\n",
        "        if model_weights != None:\n",
        "            self.load_state_dict(model_weights)\n",
        "            del model_weights\n",
        "            gc.collect()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.bptt_learning == False:\n",
        "            if self.deepspeed_stage >= 2 or self.deepspeed_offload:\n",
        "                print(f\"[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length ({self.ctx_len})\")\n",
        "        else:\n",
        "            if self.trainer.num_devices > 1:\n",
        "                if self.bptt_learning_range <= 0:\n",
        "                    print(\"[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\")\n",
        "\n",
        "        # Get the learning rate used for the optimizer\n",
        "        lr_init = self.lr_init\n",
        "        lr_final = self.lr_final\n",
        "        # If the final learning rate is not specified, use the initial learning rate\n",
        "        if lr_final < 0:\n",
        "            lr_final = self.lr_init\n",
        "\n",
        "        # Log the learning rate, and various other parameters\n",
        "        if self.trainer.local_rank == 0:\n",
        "\n",
        "            # Add the important notes, for informing users of common gotchas\n",
        "            print((\n",
        "                \"#\\n\"\n",
        "                \"# RWKV lighting_trainer.py important notes \\n\"\n",
        "                \"# https://github.com/RWKV/RWKV-infctx-trainer \\n\"\n",
        "                \"#\\n\"\n",
        "                \"# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\\n\"\n",
        "                \"# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\\n\"\n",
        "                \"# - When resuming from checkpoint, the estimated time is inaccurate\\n\"\n",
        "                \"#\"\n",
        "            ))\n",
        "\n",
        "            lr_init_e = \"{:.3e}\".format(lr_init)\n",
        "            lr_final_e = \"{:.3e}\".format(lr_final)\n",
        "            print(f\"\\n[RWKV.model] Configuring optimizer with\\n\"+\n",
        "                  f\"    - lr_init:  {lr_init_e} ({lr_init})\\n\"+\n",
        "                  f\"    - lr_final: {lr_final_e} ({lr_final})\\n\")\n",
        "\n",
        "            # Get the setup args\n",
        "            model_args = dict(self.setup_args)\n",
        "            model_args[\"__lr_init\"] = lr_init\n",
        "            model_args[\"__lr_final\"] = lr_final\n",
        "\n",
        "            # Update WANDB\n",
        "            if wandb.run is not None:\n",
        "                wandb.config.update({ \"model\": model_args })\n",
        "\n",
        "        # Setup layerwise learning rate\n",
        "        if self.layerwise_lr:\n",
        "            lr_1x = set()\n",
        "            lr_2x = set()\n",
        "            lr_3x = set()\n",
        "            for n, p in self.named_parameters():\n",
        "                if \"time_mix\" in n:\n",
        "                    lr_1x.add(n)\n",
        "                elif \"time_decay\" in n:\n",
        "                    lr_2x.add(n)\n",
        "                # V5-R2 changes\n",
        "                elif \"time_faaaa\" in n:\n",
        "                    lr_2x.add(n)\n",
        "                # elif \"time_first\" in n:\n",
        "                #     lr_3x.add(n)\n",
        "                else:\n",
        "                    lr_1x.add(n)\n",
        "            lr_1x = sorted(list(lr_1x))\n",
        "            lr_2x = sorted(list(lr_2x))\n",
        "            lr_3x = sorted(list(lr_3x))\n",
        "            # print('1x', lr_1x)\n",
        "            # print('2x', lr_2x)\n",
        "            # print('3x', lr_3x)\n",
        "            param_dict = {n: p for n, p in self.named_parameters()}\n",
        "            optim_groups = [\n",
        "                {\n",
        "                    \"params\": [param_dict[n] for n in lr_1x],\n",
        "                    \"weight_decay\": 0.0,\n",
        "                    \"lr\": 1.0 * lr_init\n",
        "                },\n",
        "                {\n",
        "                    \"params\": [param_dict[n] for n in lr_2x],\n",
        "                    \"weight_decay\": 0.0,\n",
        "                    \"lr\": 2.0 * lr_init\n",
        "                },\n",
        "                {\n",
        "                    \"params\": [param_dict[n] for n in lr_3x],\n",
        "                    \"weight_decay\": 0.0,\n",
        "                    \"lr\": 3.0 * lr_init\n",
        "                },\n",
        "            ]\n",
        "        else:\n",
        "            optim_groups = [\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.named_parameters()],\n",
        "                    \"weight_decay\": 0.0\n",
        "                },\n",
        "            ]\n",
        "\n",
        "        # Setup the adam optimizers\n",
        "        if self.deepspeed_offload:\n",
        "            optimizer = DeepSpeedCPUAdam(optim_groups,\n",
        "                                         lr=lr_init,\n",
        "                                         betas=(self.beta1, self.beta2),\n",
        "                                         eps=self.adam_eps,\n",
        "                                         bias_correction=True,\n",
        "                                         adamw_mode=False,\n",
        "                                         weight_decay=self.weight_decay,\n",
        "                                         amsgrad=False)\n",
        "        else:\n",
        "            optimizer = FusedAdam(optim_groups,\n",
        "                                  lr=lr_init,\n",
        "                                  betas=(self.beta1, self.beta2),\n",
        "                                  eps=self.adam_eps,\n",
        "                                  bias_correction=True,\n",
        "                                  adam_w_mode=False,\n",
        "                                  weight_decay=self.weight_decay,\n",
        "                                  amsgrad=False)\n",
        "\n",
        "        # Throw if wramup_steps and lr_period are both set (not supported)\n",
        "        if self.warmup_steps > 0 and self.lr_period > 0:\n",
        "            raise ValueError(\n",
        "                \"Use either warmup_steps or lr_period, not both.\")\n",
        "\n",
        "        if self.warmup_steps > 0:\n",
        "            lr_scheduler = deepspeed.runtime.lr_schedules.WarmupLR(\n",
        "                optimizer,\n",
        "                warmup_min_lr=0.2 * self.lr_init,\n",
        "                warmup_max_lr=self.lr_init,\n",
        "                warmup_num_steps=self.warmup_steps,\n",
        "                warmup_type='linear')\n",
        "\n",
        "            return {\n",
        "                'optimizer': optimizer,\n",
        "                'lr_scheduler': lr_scheduler,\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            # Skip the lr_scheduler process if lr_init and lr_final are the same\n",
        "            if lr_init == lr_final:\n",
        "                return optimizer\n",
        "\n",
        "            # The total number of steps to perform training rate decay with\n",
        "            lr_total_step = 0\n",
        "\n",
        "            # Handle lr_period -1 default behaviour of using the max_step / max_epoch\n",
        "            if self.lr_period == -1:\n",
        "                # Get trainer max_step / max_epoch\n",
        "                trainer_max_step = self.trainer.max_steps\n",
        "                trainer_max_epoch = self.trainer.max_epochs\n",
        "                if trainer_max_step > 0:\n",
        "                    lr_total_step = trainer_max_step\n",
        "                elif trainer_max_epoch > 0:\n",
        "                    lr_total_step = trainer_max_epoch * self.num_step_per_epoch()\n",
        "                else :\n",
        "                    print(\"Warning: max_step/max_epoch not set, we would be performing lr_init to lr_final shift assuming 10 epoch\")\n",
        "                    lr_total_step = 10 * self.num_step_per_epoch()\n",
        "            else:\n",
        "                # Calculate lr_total_step based on lr_period\n",
        "                if self.lr_period_type == \"step\":\n",
        "                    lr_total_step = self.lr_period\n",
        "                elif self.lr_period_type == \"epoch\":\n",
        "                    lr_total_step = self.lr_period * self.num_step_per_epoch()\n",
        "                else:\n",
        "                    raise ValueError(f\"lr_period_type {self.lr_period_type} not supported.\")\n",
        "\n",
        "            # Lets initialize the lr_scheduler\n",
        "            lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "                optimizer,\n",
        "                start_factor=1.0,\n",
        "                end_factor= lr_final / lr_init,\n",
        "                total_iters=lr_total_step\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'optimizer': optimizer,\n",
        "                'lr_scheduler': {\n",
        "                    \"scheduler\": lr_scheduler,\n",
        "                    \"interval\": \"step\",\n",
        "                    \"frequency\": 1,\n",
        "                },\n",
        "            }\n",
        "\n",
        "\n",
        "    # We have to compute the number of steps per epoch ourselves\n",
        "    # as this value is not provided directly by pytorch lightning\n",
        "    # https://github.com/Lightning-AI/lightning/issues/5449#issuecomment-1501597319\n",
        "    def num_step_per_epoch(self) -> int:\n",
        "        # Estimated number of steps in total, added as the following\n",
        "        # https://github.com/Lightning-AI/lightning/pull/11599\n",
        "        #\n",
        "        # This MUST be called before len(self.trainer.train_loader)\n",
        "        # otherwise there is a bug in which the train_dataloader is not\n",
        "        # fully initialized, which seems to be resolved by computing the\n",
        "        # self.trainer.estimated_stepping_batches\n",
        "        estimated_stepping_batches = self.trainer.estimated_stepping_batches\n",
        "\n",
        "        # Get the number of epochs,\n",
        "        # use estimated_stepping_batches if max_epochs is set\n",
        "        max_epochs = self.trainer.max_epochs\n",
        "        if max_epochs > 0:\n",
        "            return estimated_stepping_batches // max_epochs\n",
        "\n",
        "        # Get the train_dataloader\n",
        "        train_dataloader = self.trainer.train_dataloader\n",
        "        if train_dataloader is None:\n",
        "            train_dataloader = self.trainer.fit_loop._data_source.dataloader()\n",
        "\n",
        "        # Max epoch is not set, use the train_dataloader\n",
        "        dataset_size = len(train_dataloader)\n",
        "\n",
        "        num_devices = max(1, self.trainer.num_devices)\n",
        "        num_steps = dataset_size // (self.trainer.accumulate_grad_batches * num_devices)\n",
        "        return num_steps\n",
        "\n",
        "    @property\n",
        "    def deepspeed_offload(self) -> bool:\n",
        "        strategy = self.trainer.strategy\n",
        "        if isinstance(strategy, DeepSpeedStrategy):\n",
        "            cfg = strategy.config[\"zero_optimization\"]\n",
        "            return \"offload_optimizer\" in cfg or \"offload_parameters\" in cfg\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def deepspeed_stage(self) -> int:\n",
        "        strategy = self.trainer.strategy\n",
        "        if isinstance(strategy, DeepSpeedStrategy):\n",
        "            cfg = strategy.config[\"zero_optimization\"]\n",
        "            return \"stage\" in cfg\n",
        "        return -1\n",
        "\n",
        "    @TCompileBaseline\n",
        "    def forward(self, idx: torch.Tensor, last_shift_states: torch.Tensor = None,\n",
        "                last_wkv_states: torch.Tensor = None):\n",
        "        B, T, _ = idx.size()\n",
        "        assert T <= self.ctx_len, \"Cannot forward, model ctx_len is exhausted.\"\n",
        "\n",
        "        x = torch.stack([self.emb(batch_) for batch_ in idx])\n",
        "\n",
        "        # Handle dropout (input)\n",
        "        if self.dropout > 0.0:\n",
        "            x = self.drop0(x)\n",
        "\n",
        "        new_states = BlockStateList.empty(self.n_layer, B, self.n_embd,\n",
        "                                          self.n_head, self.head_size,\n",
        "                                          x.device, x.dtype)\n",
        "\n",
        "        # last_shift_states can be None, when we are performing direct inference\n",
        "        if last_shift_states is None:\n",
        "            cur_bs_list = BlockStateList.create(\n",
        "                self.n_layer, B, self.n_embd,\n",
        "                self.n_head, self.head_size,\n",
        "                x.device, x.dtype\n",
        "            )\n",
        "        else:\n",
        "            cur_bs_list = BlockStateList(last_shift_states, last_wkv_states)\n",
        "\n",
        "        # Avoid using the zip operation, as torch.compile throws an exception on it\n",
        "        # with `zip not reconized as a valid function`\n",
        "        # ---\n",
        "        # for i, (block, last_state) in enumerate(\n",
        "        #         zip(self.blocks,\n",
        "        #             BlockStateList(last_shift_states, last_wkv_states))):\n",
        "        # ---\n",
        "        for i in range(len(self.blocks)):\n",
        "            block = self.blocks[i]\n",
        "            last_state = cur_bs_list[i]\n",
        "            if self.grad_cp:\n",
        "                x, new_state = deepspeed_checkpoint(\n",
        "                    block, x, last_state)\n",
        "            else:\n",
        "                x, new_state = block(x, last_state)\n",
        "            new_states[i] = new_state\n",
        "\n",
        "        x = self.ln_out(x)\n",
        "\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x, new_states.shift_states, new_states.wkv_states\n",
        "\n",
        "    #\n",
        "    # Custom overwrite of manual_backwards operation, to skip the \"manual_backwards\"\n",
        "    # safety check, so we can perform manual backward operation step, while using\n",
        "    # the default trainer loop. This is modified from the original code found here:\n",
        "    # https://github.com/Lightning-AI/lightning/blob/37c244f94be365496def82870b22c2faf0ab889e/src/lightning/pytorch/core/module.py#L999\n",
        "    #\n",
        "    # ---\n",
        "    #\n",
        "    # This allow us to avoid disabling the \"automatic_optimization\" flag\n",
        "    #\n",
        "    # Which would have been required to do \"segmented learning\", or \"Backpropagation Through Time\"\n",
        "    # where we would need to implement manual optimization as per\n",
        "    # https://lightning.ai/docs/pytorch/stable/model/manual_optimization.html\n",
        "    #\n",
        "    # Otherwise an error will be thrown if we call `self.manual_backward`\n",
        "    #\n",
        "    # However this would mean that we would need to do a full reimplementation\n",
        "    # of several features that were handled by the automatic optimization.\n",
        "    # - accumulate_grad_batches\n",
        "    # - gradient_clip_val\n",
        "    # - logging behaviour\n",
        "    # - distributed training co-ordination\n",
        "    # - (And probably other features that I am not aware of)\n",
        "    #\n",
        "    # So this is a hacky work around, to avoid reimplementing all of the above.\n",
        "    #\n",
        "    # From the current code implementatiion, it seem like this is blocked only by\n",
        "    # automatic_optimization flag - and has no adverse side effect otherwise\n",
        "    # https://lightning.ai/docs/pytorch/stable/_modules/lightning/pytorch/core/module.html#LightningModule.manual_backward\n",
        "    #\n",
        "    # If anyone have a better idea, let me know\n",
        "    # (have experimented with, reimplementing the above, but it is not trivial, unfortunately)\n",
        "    #\n",
        "    def manual_backward(self, loss: torch.Tensor, *args, **kwargs):\n",
        "        if self._fabric:\n",
        "            self._fabric.backward(loss, *args, **kwargs)\n",
        "        else:\n",
        "            # self._verify_is_manual_optimization(\"manual_backward\")\n",
        "            self.trainer.strategy.backward(loss, None, *args, **kwargs)\n",
        "\n",
        "    #\n",
        "    # Main compute_loss function, this is called by the trainer loop\n",
        "    #\n",
        "    def compute_loss(self, batch, batch_idx, is_training_run: bool):\n",
        "        seq = batch['input_ids']\n",
        "        assert isinstance(seq, torch.Tensor) and seq.ndim == 3\n",
        "        ori_seq_mask = batch['attention_mask']\n",
        "\n",
        "        # Check if attent mask is set, if not initialize it\n",
        "        if ori_seq_mask is None or ori_seq_mask.ndim != 2:\n",
        "            ori_seq_mask = torch.ones_like(seq[:, 1:])\n",
        "\n",
        "        # Get the starting and ending loss bias\n",
        "        loss_bias_start = self.position_loss_bias\n",
        "        loss_bias_end   = 2.0 - loss_bias_start\n",
        "\n",
        "        # total_mask_sum\n",
        "        total_mask_sum = torch.sum(ori_seq_mask)\n",
        "\n",
        "        # Skip loss bias calculation, if loss_bias_start is 1.0\n",
        "        if loss_bias_start == 1.0 or (is_training_run == False and self.position_loss_bias_in_validation == False):\n",
        "            seq_mask = ori_seq_mask\n",
        "        else:\n",
        "            # Lets get a linear multiplier for the loss bias\n",
        "            # seq_mask_sum = torch.sum(ori_seq_mask)\n",
        "            bias_mask = torch.linspace(loss_bias_start, loss_bias_end, int(total_mask_sum.item()), device=ori_seq_mask.device)\n",
        "\n",
        "            # Boolean flag of seq_mask > 0\n",
        "            seq_mask_index = ori_seq_mask[0] > 0\n",
        "\n",
        "            # Apply the bias mask only to positive seq_mask values\n",
        "            final_mask = torch.zeros(ori_seq_mask.shape[1], device=ori_seq_mask.device)\n",
        "            final_mask[seq_mask_index] = ori_seq_mask[0][seq_mask_index] * bias_mask\n",
        "\n",
        "            # And save it as seq_mask\n",
        "            seq_mask = final_mask.unsqueeze(0)\n",
        "\n",
        "        # Perform cutoff for training run\n",
        "        if is_training_run:\n",
        "            prev_step = 0\n",
        "\n",
        "            # Avoid using the zip operation, as torch.compile throws an exception on it\n",
        "            # with `zip not reconized as a valid function`\n",
        "            # ---\n",
        "            # for step, len_cut in zip(self.ctx_len_warmup_steps,\n",
        "            #                          self.ctx_len_cutoffs):\n",
        "            # ---\n",
        "            for i in range(min(len(self.ctx_len_warmup_steps), len(self.ctx_len_cutoffs))):\n",
        "                step = self.ctx_len_warmup_steps[i]\n",
        "                len_cut = self.ctx_len_cutoffs[i]\n",
        "\n",
        "                if prev_step <= self.global_step < step and len_cut < seq.shape[\n",
        "                        1] - 1:\n",
        "                    pos = randint(0, seq.shape[1] - len_cut - 1)\n",
        "\n",
        "                    # Original\n",
        "                    # seq = seq[:, pos:pos + len_cut + 1]\n",
        "\n",
        "                    # Changed to use masking for prefix cutoff (i do not know if this makes sense)\n",
        "                    seq = seq[:, :pos + len_cut + 1]\n",
        "                    seq_mask = seq_mask[:, :pos + len_cut + 1]\n",
        "                    # Set the attention mask to 0 for the skipped tokens\n",
        "                    seq_mask[:, :pos] = 0\n",
        "                    # multitoken mask?\n",
        "                    print(seq_mask)\n",
        "                    seq_mask[..., seq_mask == PADDING_IDX] = 0\n",
        "                    break\n",
        "                prev_step = step\n",
        "\n",
        "        do_bptt_learning = self.bptt_learning and is_training_run\n",
        "        idx, targets = seq[:, :-1], seq[:, 1:]\n",
        "\n",
        "        B, T, _ = idx.shape\n",
        "        C = self.n_embd\n",
        "        total_mask_sum = torch.sum(seq_mask)\n",
        "\n",
        "        # If total_mask_sum, we skip, as there is no tokens of value to learn from anyway\n",
        "        if total_mask_sum == 0:\n",
        "            return 0\n",
        "\n",
        "        def multi_cross_entropy(input, target, *args, **kwargs):\n",
        "            target = target.permute(2, 0, 1)\n",
        "            return sum([F.cross_entropy(input[..., r[1][0]:r[1][1]],\n",
        "                                        (target[r[0]].view(-1) - r[1][0]) % (r[1][1] - r[1][0]), # TODO how to better prevent overflow due to padding_idx\n",
        "                                        *args, **kwargs)\n",
        "                        for r in enumerate(MULTITOKEN_RANGES)])\n",
        "\n",
        "        def checkpointed_step(idx, targets, mask, prev_loss, last_shift_states,\n",
        "                              last_wkv_states, prev_steps):\n",
        "            logits, new_shift_states, new_wkv_states = self(\n",
        "                idx, last_shift_states, last_wkv_states)\n",
        "\n",
        "            loss = multi_cross_entropy(logits.view(-1, logits.size(-1)),\n",
        "                                    targets,\n",
        "                                    reduction=\"none\")\n",
        "\n",
        "            submask = mask.view(-1)[:loss.shape[0]]\n",
        "            submask_sum = torch.sum(submask)\n",
        "            loss = torch.sum(loss * submask) / total_mask_sum\n",
        "\n",
        "            loss = L2Wrap.apply(loss, logits, total_mask_sum, submask)\n",
        "            new_steps = prev_steps + submask_sum\n",
        "            new_loss = prev_loss + loss\n",
        "            return new_loss, new_shift_states, new_wkv_states, new_steps\n",
        "\n",
        "        total_loss = torch.tensor(\n",
        "            0, dtype=self.emb.weight.dtype).requires_grad_()\n",
        "        steps = 0\n",
        "        states = BlockStateList.create(self.n_layer, B, C,\n",
        "                                       self.n_head, self.head_size,\n",
        "                                       seq.device, self.emb.weight.dtype)\n",
        "        segment_count = math.ceil(T / self.ctx_len)\n",
        "\n",
        "        #\n",
        "        # BPTT learning, we split the sequence into segments\n",
        "        # and perform a backward pass for each segment, on its own.\n",
        "        #\n",
        "        # Allowing us to perform backpropagation across context sizes much larger\n",
        "        # then what is supported by the current GPU memory.\n",
        "        #\n",
        "        # This reduces the need for the checkpointing process, and mitigate\n",
        "        # a known error where multiple backwards pass throws an exception.\n",
        "        #\n",
        "        # While not mathematically equivalent to full context size learning,\n",
        "        # it makes \"infctx\" size training possible with deepspeed 2/3\n",
        "        #\n",
        "        # ---\n",
        "        #\n",
        "        # See the following, for more details on \"Gradient computed twice\" error:\n",
        "        # https://github.com/microsoft/DeepSpeed/issues/988#issuecomment-1549417269\n",
        "        #\n",
        "        # Other possibly related issues on the topic:\n",
        "        # https://github.com/microsoft/DeepSpeed/pull/677\n",
        "        # https://github.com/EleutherAI/gpt-neox/issues/62#issuecomment-766366413\n",
        "        #\n",
        "        if do_bptt_learning:\n",
        "\n",
        "            gradient_accumulation_steps = max(1, self.trainer.accumulate_grad_batches)\n",
        "            optimizer = self.optimizers()\n",
        "            cur_device = self.device\n",
        "\n",
        "            # We use the average segment size, instead of ctx length size.\n",
        "            # this helps ensure that the segment cutoffs do not make the last segment too small.\n",
        "            # (eg, the last chunk having only 1 token)\n",
        "            #\n",
        "            # it also helps ensure the segment cutoff points are more varied, across mixed dataset sizes\n",
        "            # and avoid potentially undesired training behaviour at fixed cutoff points\n",
        "            # (this only applies for segmented learning)\n",
        "            segment_size = min(math.ceil(T / segment_count), self.ctx_len)\n",
        "\n",
        "            # Dummy 2D tenros of shape [1,1], are used to do \"dummy checkpoint/forward/backprop\" to keep everything in sync\n",
        "            dummy_2d_zero = torch.tensor([[0]], dtype=torch.long, device=cur_device)\n",
        "\n",
        "            # Get the max segment count across all GPUs, in the current batch, which is used to keep all devices are in sync\n",
        "            # Once a thread has completed all its segments, it will do dummy checkpoint/forward/backprop with one token,\n",
        "            # and stay in sync with the thread that are still working on their segments\n",
        "            #\n",
        "            # This is used to work around an undocumented behaviour for either lightning / deepspeed loss.backward multi-gpu handling\n",
        "            # where the `self.manual_backward()` / `loss.backward()` call will block / freeze / hang when being too \"out of sync\"\n",
        "            #\n",
        "            # This can be viewed as a form of `fabric.barrier()` which is invoked implicitly by each `self.manual_backward()` call\n",
        "            # except that it isn't exactly a `fabric.barrier()` - because it does not block immediately and instead blocks in\n",
        "            # the next `self.manual_backward()` call if the previous ones are too far out of sync.\n",
        "            # (its confusing, but makes sense for efficency)\n",
        "            #\n",
        "            # Additionally because the \"code line position\" and params actually matter for the 'barrier' code effect,\n",
        "            # we cant work around this issue by doing dummy `self.manual_backward()` calls, in another if/else branch or loop\n",
        "            #\n",
        "            # Combined, this makes this issue very hard to trace and debug, as it will manifest itself as randomly \"freezing\"\n",
        "            # when out of sync during `self.manual_backward()` calls. Without the current work around put in place\n",
        "            #\n",
        "            # We only do this, if we are doing bptt learning on all segments (-1), and gpu count > 1\n",
        "            # otherwise we just use the segment count as it is\n",
        "            if self.trainer.num_devices > 1:\n",
        "                if self.bptt_learning_range <= 0:\n",
        "                    # We perform forward/backward on the shared max segment count across all GPUs\n",
        "                    forward_segment_count  = self.trainer.strategy.reduce(segment_count, reduce_op=\"max\")\n",
        "                    # Convert to int, if its a torch tensor\n",
        "                    if isinstance(forward_segment_count, torch.Tensor):\n",
        "                        forward_segment_count = forward_segment_count.item()\n",
        "                    # We perform as many backward pass as we need to be equal or more then bptt_learning_range\n",
        "                    backward_segment_count = forward_segment_count\n",
        "                else:\n",
        "                    # We perform as many forward pass as we need to be equal or more then bptt_learning_range\n",
        "                    # and perform an equal amount of backward pass\n",
        "                    forward_segment_count  = max(segment_count, self.bptt_learning_range)\n",
        "                    backward_segment_count = self.bptt_learning_range\n",
        "            else:\n",
        "                if self.bptt_learning_range <= 0:\n",
        "                    # Since we do not need to sync GPUs here, we perform as much forward as we exactly need\n",
        "                    forward_segment_count  = segment_count\n",
        "                    backward_segment_count = forward_segment_count\n",
        "                else:\n",
        "                    # We clamp the backward segment count to the forward count, and bptt_learning_range\n",
        "                    forward_segment_count  = segment_count\n",
        "                    backward_segment_count = min(self.bptt_learning_range, segment_count)\n",
        "\n",
        "            # We compute when we start the segmented learning process\n",
        "            if forward_segment_count != backward_segment_count:\n",
        "                start_learning_segment = max(segment_count - self.bptt_learning_range, 0);\n",
        "            else:\n",
        "                start_learning_segment = 0;\n",
        "\n",
        "            # # Segment loss array to track (and reduce later)\n",
        "            # # of size equal to forward_segment_count\n",
        "            # segment_loss_arr = [0] * forward_segment_count\n",
        "\n",
        "            # Lets go through and forward all the segments\n",
        "            # (including dummy ones)\n",
        "            for i in range(forward_segment_count):\n",
        "                # Apply state truncation, if truncated learning is enabled\n",
        "                # this limits the backprop process, reduces loss learning rate,\n",
        "                # but save vram across extreamly large backpropagation steps\n",
        "                if self.bptt_truncated_learning:\n",
        "                    prv_shift_states = states.shift_states.clone().detach().requires_grad_(False)\n",
        "                    prv_wkv_states = states.wkv_states.clone().detach().requires_grad_(False)\n",
        "                else:\n",
        "                    prv_shift_states = states.shift_states\n",
        "                    prv_wkv_states = states.wkv_states\n",
        "\n",
        "                # We use a dummy masked token 0, to do additional dummy checkpoint/forward/backprop when needed\n",
        "                # for each additional call after the current \"segment_count\" max\n",
        "                if i <= segment_count - 1:\n",
        "                    cur_idx = idx[:, i * segment_size:(i + 1) * segment_size]\n",
        "                    cur_tar = targets[:, i * segment_size:(i + 1) * segment_size]\n",
        "                    cur_msk = seq_mask[:, i * segment_size:(i + 1) * segment_size]\n",
        "                else:\n",
        "                    cur_idx = dummy_2d_zero\n",
        "                    cur_tar = dummy_2d_zero\n",
        "                    cur_msk = dummy_2d_zero\n",
        "\n",
        "                # Segmented learning, applies the forward/pass over each chunk seperately\n",
        "                segment_loss, new_shift_states, new_wkv_states, steps = checkpointed_step(\n",
        "                    cur_idx,\n",
        "                    cur_tar,\n",
        "                    cur_msk,\n",
        "                    torch.tensor(0, dtype=self.emb.weight.dtype, device=cur_device).requires_grad_(True),\n",
        "                    prv_shift_states,\n",
        "                    prv_wkv_states,\n",
        "                    steps,\n",
        "                )\n",
        "                states = BlockStateList(new_shift_states, new_wkv_states)\n",
        "\n",
        "                # # Keep the segment loss (for backpassing in reverse)\n",
        "                # segment_loss_arr[i] = segment_loss\n",
        "\n",
        "                # Perform the backward pass accordingly, for valid segments (besides the last segment)\n",
        "                # In this version, we do backward passes together the forward passes in the main segment loop\n",
        "                # Instead of after all segment losses are computed\n",
        "                if i >= start_learning_segment and i < start_learning_segment + backward_segment_count:\n",
        "                    # The learning loss, should be normalized against the accumulation steps\n",
        "                    # as we are bypassing the pytorch lightning normalization\n",
        "                    # https://lightning.ai/docs/pytorch/2.0.4/common/lightning_module.html#backward\n",
        "                    learning_loss = segment_loss / gradient_accumulation_steps\n",
        "\n",
        "                    # Perform the backward pass accordingly, for valid segments (besides the last segment)\n",
        "                    if i == start_learning_segment + backward_segment_count - 1:\n",
        "                        # This is the last backward pass, we let the default pytorch lightning handle the backward pass\n",
        "                        # and return the segment loss as part of the total loss\n",
        "                        total_loss = total_loss + segment_loss\n",
        "                    else:\n",
        "                        # Undocumented multiple backward pass support\n",
        "                        # https://github.com/Lightning-AI/lightning/blob/678f642808c54e4c490caee4df5d357301c976bb/tests/trainer/optimization/test_manual_optimization.py#L251\n",
        "                        self.manual_backward(learning_loss, optimizer, retain_graph=True)\n",
        "\n",
        "                        # Accumulate without gradient, as we already did the backward pass\n",
        "                        total_loss = total_loss + segment_loss.clone().detach().requires_grad_(False)\n",
        "                else:\n",
        "                    # Even if its not the segments we use for backward pass, we still need to accumulate the loss\n",
        "                    total_loss = total_loss + segment_loss.clone().detach().requires_grad_(False)\n",
        "\n",
        "                # GC collect unused memory\n",
        "                # gc.collect()\n",
        "                # torch.cuda.empty_cache()\n",
        "\n",
        "            # # Lets backpass the respective segments, in reverse\n",
        "            # # (including dummy backpass)\n",
        "            # for i in range(forward_segment_count-1, -1, -1):\n",
        "            #     # Get the segment loss\n",
        "            #     segment_loss = segment_loss_arr[i]\n",
        "            #\n",
        "            #     # Compute the backward pass for the segment\n",
        "            #     if i >= start_learning_segment and i < start_learning_segment + backward_segment_count:\n",
        "            #         # The learning loss, should be normalized against the accumulation steps\n",
        "            #         # as we are bypassing the pytorch lightning normalization\n",
        "            #         # https://lightning.ai/docs/pytorch/2.0.4/common/lightning_module.html#backward\n",
        "            #         learning_loss = segment_loss / gradient_accumulation_steps\n",
        "            #\n",
        "            #         # Perform the backward pass accordingly, for valid segments (besides the start_learning_segment)\n",
        "            #         if i > start_learning_segment:\n",
        "            #             # Undocumented multiple backward pass support\n",
        "            #             # https://github.com/Lightning-AI/lightning/blob/678f642808c54e4c490caee4df5d357301c976bb/tests/trainer/optimization/test_manual_optimization.py#L251\n",
        "            #             self.manual_backward(learning_loss, optimizer, retain_graph=True)\n",
        "            #\n",
        "            #             # Accumulate without gradient, as we already did the backward pass\n",
        "            #             total_loss = total_loss + segment_loss.clone().detach().requires_grad_(False)\n",
        "            #         else:\n",
        "            #             # This is the last backward pass, we let the default pytorch lightning handle the backward pass\n",
        "            #             # and return the segment loss as part of the total loss\n",
        "            #             total_loss = total_loss + segment_loss\n",
        "            #     else:\n",
        "            #         # Even if its not the segments we use for backward pass, we still need to accumulate the loss\n",
        "            #         total_loss = total_loss + segment_loss.clone().detach().requires_grad_(False)\n",
        "            #\n",
        "            #    # GC collect unused memory\n",
        "            #    gc.collect()\n",
        "            #    # torch.cuda.empty_cache()\n",
        "        else:\n",
        "\n",
        "            # Normal operations without BPTT\n",
        "            segment_size = self.ctx_len\n",
        "            for i in range(segment_count):\n",
        "                if i < segment_count-1 and is_training_run:\n",
        "                    total_loss, new_shift_states, new_wkv_states, steps = deepspeed_checkpoint(\n",
        "                        checkpointed_step,\n",
        "                        idx[:, i * segment_size:(i + 1) * segment_size],\n",
        "                        targets[:, i * segment_size:(i + 1) * segment_size],\n",
        "                        seq_mask[:, i * segment_size:(i + 1) * segment_size],\n",
        "                        total_loss,\n",
        "                        states.shift_states,\n",
        "                        states.wkv_states,\n",
        "                        steps,\n",
        "                    )\n",
        "                else:\n",
        "                    total_loss, new_shift_states, new_wkv_states, steps = checkpointed_step(\n",
        "                        idx[:, i * segment_size:(i + 1) * segment_size],\n",
        "                        targets[:, i * segment_size:(i + 1) * segment_size],\n",
        "                        seq_mask[:, i * segment_size:(i + 1) * segment_size],\n",
        "                        total_loss,\n",
        "                        states.shift_states,\n",
        "                        states.wkv_states,\n",
        "                        steps,\n",
        "                    )\n",
        "\n",
        "                states = BlockStateList(new_shift_states, new_wkv_states)\n",
        "                gc.collect()\n",
        "                # torch.cuda.empty_cache()\n",
        "\n",
        "        # Wandb logging only, if an active run exists\n",
        "        if wandb.run is not None:\n",
        "            global_rank = self.global_rank\n",
        "            global_device_count = self.trainer.num_devices * self.trainer.num_nodes\n",
        "            wandb.log({\n",
        "                'substep': batch_idx * global_device_count + global_rank,\n",
        "                'batchidx': batch_idx,\n",
        "                'global_rank': global_rank,\n",
        "                'real_ctx_len': T,\n",
        "                'train/loss': total_loss,\n",
        "                'trainer/global_step':self.global_step,\n",
        "                'trainer/learning_rate': self.trainer.optimizers[0].param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    @TCompileBaseline\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        total_loss = self.compute_loss(batch, batch_idx, True)\n",
        "\n",
        "        self.log('train/loss', total_loss, prog_bar=True)\n",
        "        # If set - forces the above train/loss log line to always be on a new line\n",
        "        if self.substep_logging:\n",
        "            print(\"\")\n",
        "\n",
        "        if self.substep_cuda_cache_clear:\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    @TCompileBaseline\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        total_loss = self.compute_loss(batch, batch_idx, False)\n",
        "        self.log('validation/loss', total_loss, prog_bar=True, sync_dist=True)\n",
        "        return total_loss\n",
        "\n",
        "########################################################################################################\n",
        "# SimpleRWKV, a wrapper for RWKV that allows for simple usage of the model\n",
        "########################################################################################################\n",
        "\n",
        "# SimpleRWKV specific imports\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Current script dir\n",
        "SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
        "SCRIPT_PARENT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, '../'))\n",
        "\n",
        "# SimpleRWKV is a wrapper for RWKV that allows for simple usage of the model\n",
        "#\n",
        "# it is not meant to be highly performant, but rather a simple minimal way to run the RWKV trainer module\n",
        "# in inference mode, and can be used to validate the model trainer code / its changes\n",
        "class SimpleRWKV():\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_path: str,\n",
        "            ctx_len:int = 1024,\n",
        "            device:str = \"cuda\",\n",
        "            dtype:str = \"fp32\"\n",
        "        ):\n",
        "\n",
        "        # Log the mismatch dtype\n",
        "        if dtype != \"fp32\":\n",
        "            print(\"[SimpleRWKV] Warning: dtype mismatch, only fp32 is supported (for now)\")\n",
        "\n",
        "        # Prepare the model config with the model path, and custom torch load\n",
        "        model_config = {}\n",
        "        model_config[\"load_model\"] = model_path\n",
        "        model_config[\"ctx_len\"] = ctx_len\n",
        "\n",
        "        # This feature depends on deepspeed\n",
        "        model_config[\"grad_cp\"] = False\n",
        "        # model_config[\"_torch_load_state\"] = loaded_state\n",
        "\n",
        "        # Save the config settings\n",
        "        self.ctx_len = ctx_len\n",
        "        self.device = device\n",
        "\n",
        "        # Lets actually load the model\n",
        "        self.model = RWKV(**model_config)\n",
        "\n",
        "        # Lets map it over to the respective device type\n",
        "        # and set it to run as eval/inference mode\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Get the model detected vocab size\n",
        "        vocab_size = self.model.vocab_size\n",
        "\n",
        "        # The tokenizer object values\n",
        "        self.fastTokenizer = None\n",
        "        self.worldTokenizer = None\n",
        "\n",
        "        # Setup the tokenizer\n",
        "        if vocab_size == 50277:\n",
        "            # Use the neox tokenizer\n",
        "            tokenizer_file = os.path.join(SCRIPT_DIR,\"./dataflow/20B_tokenizer.json\")\n",
        "            tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
        "            self.fastTokenizer = tokenizer\n",
        "        elif vocab_size == 65536:\n",
        "            # Use the world tokenizer\n",
        "            from .dataflow.trie_tokenizer import MT_TRIE_TOKENIZER\n",
        "            world_tokenizer = MT_TRIE_TOKENIZER(os.path.join(SCRIPT_DIR, \"./dataflow/rwkv_vocab_v20230424.txt\"))\n",
        "            self.worldTokenizer = world_tokenizer\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unsupported vocab size ({vocab_size}) - custom tokenizer not supported\")\n",
        "\n",
        "    # Encoding strings\n",
        "    def encode(self, text: str):\n",
        "        if self.worldTokenizer != None:\n",
        "            return self.worldTokenizer.encode(text)\n",
        "        return self.fastTokenizer.encode(text)\n",
        "\n",
        "    # Decoding strings\n",
        "    def decode(self, tokens: list):\n",
        "        if self.worldTokenizer != None:\n",
        "            return self.worldTokenizer.decode(tokens)\n",
        "        return self.fastTokenizer.decode(tokens)\n",
        "\n",
        "    # Forwarding logic, withoout torch._no_grad() context\n",
        "    def _forward(\n",
        "            self, tokens,\n",
        "            stateObj = None,\n",
        "            all_logits = False\n",
        "        ):\n",
        "\n",
        "        logits_arr = None\n",
        "        token_len = len(tokens)\n",
        "\n",
        "        # Get the shift/wkv state\n",
        "        if stateObj is None:\n",
        "            shift_states = None\n",
        "            wkv_states = None\n",
        "        else:\n",
        "            shift_states = stateObj[\"shift_states\"]\n",
        "            wkv_states = stateObj[\"wkv_states\"]\n",
        "\n",
        "        # The all_logits array, if requested\n",
        "        all_logits_arr = None\n",
        "\n",
        "        # For each token, process the state, in batches up to ctx_len\n",
        "        for i in range(0, token_len, self.ctx_len):\n",
        "            # Token set\n",
        "            token_set = tokens[i:i+self.ctx_len]\n",
        "\n",
        "            # Check if tokens are already tensors\n",
        "            batch_tokens = torch.tensor(\n",
        "                token_set,\n",
        "                dtype=torch.long, device=self.device\n",
        "            ).unsqueeze(0)\n",
        "\n",
        "            # Compute the logits and state\n",
        "            logits_arr, shift_states, wkv_states = self.model.forward(\n",
        "                batch_tokens, shift_states, wkv_states\n",
        "            )\n",
        "\n",
        "            # Build the all_logits array\n",
        "            if all_logits:\n",
        "                if all_logits_arr is None:\n",
        "                    all_logits_arr = logits_arr[0]\n",
        "                else:\n",
        "                    all_logits_arr = torch.cat([all_logits_arr, logits_arr[0]], dim=0)\n",
        "\n",
        "        # Return the logits and state\n",
        "        if all_logits:\n",
        "            return all_logits_arr, { \"shift_states\": shift_states, \"wkv_states\": wkv_states }\n",
        "        else:\n",
        "            return logits_arr[0][-1], { \"shift_states\": shift_states, \"wkv_states\": wkv_states }\n",
        "\n",
        "    # Forwarding logic, with torch._no_grad() context\n",
        "    def forward(\n",
        "            self, tokens:list,\n",
        "            stateObj = None,\n",
        "            all_logits = False\n",
        "        ):\n",
        "        with torch.no_grad():\n",
        "            return self._forward(tokens, stateObj, all_logits)\n",
        "\n",
        "    # Sampling logits\n",
        "    def sample_logits(\n",
        "            self, logits,\n",
        "            prv_tokens=[0],\n",
        "            temperature=1.0, top_p=0.9,\n",
        "            token_ban: list = []\n",
        "            ):\n",
        "        # Copy to CPU first\n",
        "        logits = logits.cpu()\n",
        "\n",
        "        # Max negative float\n",
        "        max_neg = -torch.finfo(torch.float).max\n",
        "\n",
        "        # Apply token ban\n",
        "        for x in token_ban:\n",
        "            logits[x] = max_neg\n",
        "\n",
        "        # Remove NaNs from logits\n",
        "        for x in range(len(logits)):\n",
        "            if torch.isnan(logits[x]):\n",
        "                logits[x] = max_neg\n",
        "\n",
        "        # Handle sampling with temperature\n",
        "        if temperature > 0.0:\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            sorted_probs = torch.sort(probs, descending=True)[0]\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1).cpu().numpy()\n",
        "            cutoff = float(sorted_probs[np.argmax(cumulative_probs > top_p)])\n",
        "            probs[probs < cutoff] = 0\n",
        "            if temperature != 1.0:\n",
        "                probs = probs.pow(1.0 / temperature)\n",
        "            out = torch.multinomial(probs, num_samples=1)[0]\n",
        "            return out\n",
        "        else:\n",
        "            # Since the tokenizer sample does not support temp==0\n",
        "            # we handle this case ourself, by fining the top token\n",
        "            return torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Completion API\n",
        "    def completion(self,\n",
        "            prompt,\n",
        "            max_tokens: int = 32,\n",
        "            temperature: float = 1.0,\n",
        "            top_p: float = 0.9,\n",
        "            token_ban: list = [],\n",
        "            start_state = None,\n",
        "            stream_to_stdout: bool = False,\n",
        "        ):\n",
        "        # Encode the context, if its a string\n",
        "        if isinstance(prompt, str):\n",
        "            enc = self.encode(prompt)\n",
        "        # Check if the prompt is a list of tokens\n",
        "        elif isinstance(prompt, list):\n",
        "            enc = prompt\n",
        "        else:\n",
        "            raise ValueError(\"Prompt must be a string or a list of tokens\")\n",
        "\n",
        "        # Keep track of the logits and state\n",
        "        logits = None\n",
        "        stateObj = start_state\n",
        "\n",
        "        # For each token, process the state\n",
        "        logits, stateObj = self.forward(enc, stateObj)\n",
        "\n",
        "        # # Garbage collect\n",
        "        # gc.collect()\n",
        "        # torch.cuda.empty_cache()\n",
        "\n",
        "        # Generate each token\n",
        "        out_tokens = []\n",
        "        for i in range(max_tokens):\n",
        "            ttt = self.sample_logits(\n",
        "                logits,\n",
        "                # prv_tokens=full_tokens,\n",
        "                temperature=temperature, top_p=top_p,\n",
        "                token_ban=token_ban\n",
        "            )\n",
        "\n",
        "            # Append the token\n",
        "            out_tokens.append(ttt)\n",
        "            # full_tokens.append(ttt)\n",
        "            if stream_to_stdout:\n",
        "                print(self.decode([ttt]), end=\"\", flush=True)\n",
        "\n",
        "            # Perform the forward pass\n",
        "            logits, stateObj = self.forward([ttt], stateObj)\n",
        "\n",
        "        # Decode the tokens\n",
        "        out_str = self.decode(out_tokens)\n",
        "\n",
        "        # # Garbage collect\n",
        "        # gc.collect()\n",
        "        # torch.cuda.empty_cache()\n",
        "\n",
        "        # Return the output string, and state\n",
        "        return out_str, stateObj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eILbvbqCXI0J",
        "outputId": "4567a797-feb7-4d4b-e71a-a43092a3e9c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/RWKV-infctx-trainer/RWKV-v5/src/trainer.py\n"
          ]
        }
      ],
      "source": [
        "#@title patch trainer.py\n",
        "\n",
        "%%writefile /content/RWKV-infctx-trainer/RWKV-v5/src/trainer.py\n",
        "\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.strategies.deepspeed import DeepSpeedStrategy\n",
        "import lightning as Lightning\n",
        "import torch\n",
        "import math\n",
        "import wandb\n",
        "\n",
        "# We extend the native pytorch lightning trainer to add the following\n",
        "#\n",
        "# - local \"fabric\" support, as the trainer object is one of the few\n",
        "#   objects that is available to all the processess\n",
        "# - target_batch_size, which automatically computes the accumulate_grad_batches\n",
        "class RWKVLightningTrainer(Trainer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            # Replaces the accumulate_grad_batches, if set\n",
        "            # automatically compute the accumulate_grad_batches\n",
        "            # according to the num_nodes, and num_devices configured\n",
        "            target_batch_size=-1,\n",
        "            # Handle the rest of args, as per normal\n",
        "            **kwargs,\n",
        "        ):\n",
        "\n",
        "        # trainer_config args (used for wndb logging)\n",
        "        trainer_config = dict(kwargs)\n",
        "\n",
        "        # target batch size logging\n",
        "        target_batch_size_log_msg = \"\"\n",
        "\n",
        "        # Compute the accumulate_grad_batches, using the target_batch_size\n",
        "        self.target_batch_size = target_batch_size\n",
        "        if target_batch_size > 0:\n",
        "\n",
        "            # Check if the accumulate_grad_batches is already set\n",
        "            # (note that it seems that pytorch lightning defaults to 1)\n",
        "            if \"accumulate_grad_batches\" in kwargs and kwargs[\"accumulate_grad_batches\"] > 1:\n",
        "                raise ValueError(f\"Cannot set both 'target_batch_size' ({target_batch_size}) and 'accumulate_grad_batches' ({kwargs['accumulate_grad_batches']}))\")\n",
        "\n",
        "            # Extract the num_nodes and devices\n",
        "            num_nodes = kwargs.get(\"num_nodes\", 1)\n",
        "            devices = kwargs.get(\"devices\", \"auto\")\n",
        "\n",
        "            # Compute the number of devices\n",
        "            if devices == \"auto\":\n",
        "                num_devices = torch.cuda.device_count()\n",
        "            elif isinstance(devices, int):\n",
        "                num_devices = devices\n",
        "            elif isinstance(devices, list):\n",
        "                num_devices = len(devices)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported devices config '{devices}', unable to compute device count for 'target_batch_size'\")\n",
        "\n",
        "            # Compute the accumulate_grad_batches\n",
        "            accumulate_grad_batches = max( 1, math.floor(target_batch_size / (num_nodes * num_devices)) )\n",
        "            kwargs[\"accumulate_grad_batches\"] = accumulate_grad_batches\n",
        "            effective_batch_size = accumulate_grad_batches * num_nodes * num_devices\n",
        "\n",
        "            # Log the applied accumulate_grad_batches\n",
        "            trainer_config[\"__accumulate_grad_batches\"] = accumulate_grad_batches\n",
        "            trainer_config[\"__effective_batch_size\"] = effective_batch_size\n",
        "\n",
        "            # Log the computed accumulate_grad_batches\n",
        "            # this is done after _init_ so we can confirm local rank\n",
        "            target_batch_size_log_msg = (\"\\n\"+\n",
        "                f\"\\n[RWKV.Trainer] Applying 'target_batch_size' with the following:\\n\"+\n",
        "                f\"   - target_batch_size:       {target_batch_size}\\n\"+\n",
        "                f\"   - num_nodes:               {num_nodes}\\n\"+\n",
        "                f\"   - num_devices:             {num_devices}\\n\"+\n",
        "                f\"   - accumulate_grad_batches: {accumulate_grad_batches}\\n\"\n",
        "                f\"   - effective_batch_size:    {effective_batch_size}\\n\")\n",
        "\n",
        "        # Update WANDB config\n",
        "        # ---\n",
        "        if wandb.run is not None:\n",
        "            trainer_config[\"target_batch_size\"] = target_batch_size\n",
        "            del trainer_config[\"logger\"]\n",
        "            del trainer_config[\"callbacks\"]\n",
        "            wandb.config.update({\n",
        "                \"trainer\": trainer_config\n",
        "            })\n",
        "\n",
        "        # Call the parent constructor\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._fabric_instance = None\n",
        "\n",
        "        # Log the target_batch_size_log_msg\n",
        "        # if local rank is 0\n",
        "        if target_batch_size_log_msg != \"\" and self.local_rank == 0:\n",
        "            print(target_batch_size_log_msg)\n",
        "\n",
        "    # Fabric instance, useful for coordinating between processes\n",
        "    # when `self.trainer.strategy.reduce` is not possible\n",
        "    def getFabric(self):\n",
        "        if self._fabric_instance is not None:\n",
        "            return self._fabric_instance\n",
        "\n",
        "        strat = self.strategy\n",
        "        if strat is None:\n",
        "            raise ValueError(\"Trainer strategy config is missing\")\n",
        "\n",
        "        # Map the pytorch lightning strat to fabric strat string\n",
        "        stratStr = \"auto\"\n",
        "        if isinstance(strat, DeepSpeedStrategy):\n",
        "            stratStr = \"deepspeed\"\n",
        "\n",
        "        self._fabric_instance = Lightning.Fabric(\n",
        "            accelerator=self.accelerator,\n",
        "            devices=self.num_devices,\n",
        "            num_nodes=self.num_nodes,\n",
        "            strategy=stratStr\n",
        "        )\n",
        "        return self._fabric_instance\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade protobuf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovhFRvQFCQIh",
        "outputId": "8c5de621-046e-4477-869f-493092253242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.19.6)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-4.24.2-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "descript-audiotools 0.7.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.24.2 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.24.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4svE_h2bd0Bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7917744-411c-4992-a660-f7d36048b6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-06 11:57:33.066302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-09-06 11:57:35,439] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
            "---- Initializing model ----\n",
            "No of layers: 12\n",
            "Embedding size: 1024\n",
            "Output model path: /content/init.pth\n",
            "Vocab size: 65536\n",
            "Emb scale: 0.0001\n",
            "Note: this process takes a significant time (and ram) for large models\n",
            "---- ----- ----\n",
            "65536 1024  -0.0001 emb.weight\n",
            "1024  1024  1.0  blocks.0.att.receptance.weight\n",
            "1024  1024  1.0  blocks.0.att.key.weight\n",
            "1024  1024  1.0  blocks.0.att.value.weight\n",
            "1024  1024  0    blocks.0.att.output.weight\n",
            "4096  1024  1.0  blocks.0.ffn.key.weight\n",
            "1024  1024  0    blocks.0.ffn.receptance.weight\n",
            "1024  4096  0    blocks.0.ffn.value.weight\n",
            "1024  1024  1.0  blocks.1.att.receptance.weight\n",
            "1024  1024  1.0  blocks.1.att.key.weight\n",
            "1024  1024  1.0  blocks.1.att.value.weight\n",
            "1024  1024  0    blocks.1.att.output.weight\n",
            "4096  1024  1.0  blocks.1.ffn.key.weight\n",
            "1024  1024  0    blocks.1.ffn.receptance.weight\n",
            "1024  4096  0    blocks.1.ffn.value.weight\n",
            "1024  1024  1.0  blocks.2.att.receptance.weight\n",
            "1024  1024  1.0  blocks.2.att.key.weight\n",
            "1024  1024  1.0  blocks.2.att.value.weight\n",
            "1024  1024  0    blocks.2.att.output.weight\n",
            "4096  1024  1.0  blocks.2.ffn.key.weight\n",
            "1024  1024  0    blocks.2.ffn.receptance.weight\n",
            "1024  4096  0    blocks.2.ffn.value.weight\n",
            "1024  1024  1.0  blocks.3.att.receptance.weight\n",
            "1024  1024  1.0  blocks.3.att.key.weight\n",
            "1024  1024  1.0  blocks.3.att.value.weight\n",
            "1024  1024  0    blocks.3.att.output.weight\n",
            "4096  1024  1.0  blocks.3.ffn.key.weight\n",
            "1024  1024  0    blocks.3.ffn.receptance.weight\n",
            "1024  4096  0    blocks.3.ffn.value.weight\n",
            "1024  1024  1.0  blocks.4.att.receptance.weight\n",
            "1024  1024  1.0  blocks.4.att.key.weight\n",
            "1024  1024  1.0  blocks.4.att.value.weight\n",
            "1024  1024  0    blocks.4.att.output.weight\n",
            "4096  1024  1.0  blocks.4.ffn.key.weight\n",
            "1024  1024  0    blocks.4.ffn.receptance.weight\n",
            "1024  4096  0    blocks.4.ffn.value.weight\n",
            "1024  1024  1.0  blocks.5.att.receptance.weight\n",
            "1024  1024  1.0  blocks.5.att.key.weight\n",
            "1024  1024  1.0  blocks.5.att.value.weight\n",
            "1024  1024  0    blocks.5.att.output.weight\n",
            "4096  1024  1.0  blocks.5.ffn.key.weight\n",
            "1024  1024  0    blocks.5.ffn.receptance.weight\n",
            "1024  4096  0    blocks.5.ffn.value.weight\n",
            "1024  1024  1.0  blocks.6.att.receptance.weight\n",
            "1024  1024  1.0  blocks.6.att.key.weight\n",
            "1024  1024  1.0  blocks.6.att.value.weight\n",
            "1024  1024  0    blocks.6.att.output.weight\n",
            "4096  1024  1.0  blocks.6.ffn.key.weight\n",
            "1024  1024  0    blocks.6.ffn.receptance.weight\n",
            "1024  4096  0    blocks.6.ffn.value.weight\n",
            "1024  1024  1.0  blocks.7.att.receptance.weight\n",
            "1024  1024  1.0  blocks.7.att.key.weight\n",
            "1024  1024  1.0  blocks.7.att.value.weight\n",
            "1024  1024  0    blocks.7.att.output.weight\n",
            "4096  1024  1.0  blocks.7.ffn.key.weight\n",
            "1024  1024  0    blocks.7.ffn.receptance.weight\n",
            "1024  4096  0    blocks.7.ffn.value.weight\n",
            "1024  1024  1.0  blocks.8.att.receptance.weight\n",
            "1024  1024  1.0  blocks.8.att.key.weight\n",
            "1024  1024  1.0  blocks.8.att.value.weight\n",
            "1024  1024  0    blocks.8.att.output.weight\n",
            "4096  1024  1.0  blocks.8.ffn.key.weight\n",
            "1024  1024  0    blocks.8.ffn.receptance.weight\n",
            "1024  4096  0    blocks.8.ffn.value.weight\n",
            "1024  1024  1.0  blocks.9.att.receptance.weight\n",
            "1024  1024  1.0  blocks.9.att.key.weight\n",
            "1024  1024  1.0  blocks.9.att.value.weight\n",
            "1024  1024  0    blocks.9.att.output.weight\n",
            "4096  1024  1.0  blocks.9.ffn.key.weight\n",
            "1024  1024  0    blocks.9.ffn.receptance.weight\n",
            "1024  4096  0    blocks.9.ffn.value.weight\n",
            "1024  1024  1.0  blocks.10.att.receptance.weight\n",
            "1024  1024  1.0  blocks.10.att.key.weight\n",
            "1024  1024  1.0  blocks.10.att.value.weight\n",
            "1024  1024  0    blocks.10.att.output.weight\n",
            "4096  1024  1.0  blocks.10.ffn.key.weight\n",
            "1024  1024  0    blocks.10.ffn.receptance.weight\n",
            "1024  4096  0    blocks.10.ffn.value.weight\n",
            "1024  1024  1.0  blocks.11.att.receptance.weight\n",
            "1024  1024  1.0  blocks.11.att.key.weight\n",
            "1024  1024  1.0  blocks.11.att.value.weight\n",
            "1024  1024  0    blocks.11.att.output.weight\n",
            "4096  1024  1.0  blocks.11.ffn.key.weight\n",
            "1024  1024  0    blocks.11.ffn.receptance.weight\n",
            "1024  4096  0    blocks.11.ffn.value.weight\n",
            "65536 1024  0.5  head.weight\n"
          ]
        }
      ],
      "source": [
        "!python /content/RWKV-infctx-trainer/RWKV-v5/init_model.py --n_layer 12 --n_embd 1024 --vocab_size 65536 --skip-if-exists /content/init.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0EsT9fSfuWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebe9195-d34c-43a7-c5e8-50d73fb04c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/RWKV-infctx-trainer/RWKV-v5/HF': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/RWKV-infctx-trainer/RWKV-v5/HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhPpAj_gcfBo",
        "outputId": "fe619cc1-ed06-4530-bc8d-3ca2b8be6301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RWKV-infctx-trainer/RWKV-v5\n",
            "2023-09-06 11:58:54.179300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-09-06 11:58:56,375] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/content/config.yaml', '--seed', '150'], args=['fit', '-c', '/content/config.yaml', '--seed', '150'].\n",
            "  rank_zero_warn(\n",
            "Global seed set to 150\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230906_115917-hl9z25kr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRWKV_train_music_2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/leavemealone/RWKV_training_music_2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/leavemealone/RWKV_training_music_2/runs/hl9z25kr\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/connector.py:554: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "  rank_zero_warn(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "\n",
            "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
            "   - target_batch_size:       3\n",
            "   - num_nodes:               1\n",
            "   - num_devices:             1\n",
            "   - accumulate_grad_batches: 3\n",
            "   - effective_batch_size:    3\n",
            "\n",
            "Generating train split: 3442 examples [03:09, 18.15 examples/s]\n",
            "Saving the dataset (20/20 shards): 100% 3407/3407 [00:34<00:00, 97.92 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 35/35 [00:00<00:00, 75.17 examples/s]\n",
            "[rank: 0] Global seed set to 150\n",
            "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "[2023-09-06 12:03:09,667] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "Enabling DeepSpeed BF16.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "#\n",
            "# RWKV lighting_trainer.py important notes \n",
            "# https://github.com/RWKV/RWKV-infctx-trainer \n",
            "#\n",
            "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
            "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
            "# - When resuming from checkpoint, the estimated time is inaccurate\n",
            "#\n",
            "\n",
            "[RWKV.model] Configuring optimizer with\n",
            "    - lr_init:  3.000e-04 (0.0003)\n",
            "    - lr_final: 8.000e-05 (8e-05)\n",
            "\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
            "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
            "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 37.150354623794556 seconds\n",
            "Loading `train_dataloader` to estimate number of stepping batches.\n",
            "Rank: 0 partition count [1, 1] and sizes[(297934848, False), (384, False)] \n",
            "\n",
            "  | Name   | Type         | Params\n",
            "----------------------------------------\n",
            "0 | emb    | EmbeddingBag | 67.1 M\n",
            "1 | blocks | ModuleList   | 163 M \n",
            "2 | ln_out | LayerNorm    | 2.0 K \n",
            "3 | head   | Linear       | 67.1 M\n",
            "----------------------------------------\n",
            "297 M     Trainable params\n",
            "0         Non-trainable params\n",
            "297 M     Total params\n",
            "1,191.741 Total estimated model params size (MB)\n",
            "Epoch 0:   9% 300/3407 [1:30:09<15:33:44, 18.03s/it, v_num=25kr, train/loss=251.0]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  warnings.warn(\n",
            "Epoch 0:  20% 680/3407 [3:30:51<14:05:36, 18.61s/it, v_num=25kr, train/loss=248.0]"
          ]
        }
      ],
      "source": [
        "%cd /content/RWKV-infctx-trainer/RWKV-v5\n",
        "!python lightning_trainer.py fit -c /content/config.yaml --seed 150"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7WAluBZ1lTXS",
        "3xtUV7iTaYsQ"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}